{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/GD-NLP/blob/main/%5BGD_14%5D_BERT_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da392d6f",
      "metadata": {
        "id": "da392d6f"
      },
      "source": [
        "# 14-8. 프로젝트 : mini BERT 만들기\n",
        "\n",
        "- vocab size를 8000으로 줄이고, 전체 파라미터 사이즈가 1M 정도가 되는 아주 작은 mini BERT 모델을 만들어 10 Epoch까지 학습시킨 모델을 만들어 보는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6c0271",
      "metadata": {
        "scrolled": true,
        "id": "6e6c0271",
        "outputId": "6258438e-dd02-4a0f-cad7-842d53de3b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "1.21.4\n",
            "1.3.3\n",
            "2.0.9\n",
            "2.2.1\n"
          ]
        }
      ],
      "source": [
        "# 라이브러리 불러오기\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import collections\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "import copy\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sentencepiece as spm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "random_seed = 1234\n",
        "random.seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "print(tf.__version__)\n",
        "print(np.__version__)\n",
        "print(pd.__version__)\n",
        "print(json.__version__)\n",
        "print(re.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d20d87d",
      "metadata": {
        "id": "6d20d87d"
      },
      "source": [
        "# 1. Tokenizer 준비\n",
        "\n",
        "- SentencePiece 모델을 이용해 BERT의 MLM 학습용 데이터를 만드세요.\n",
        "- 이를 위해 한글 나무 위키 코퍼스로부터 8000의 vocab_size를 갖는 sentencepiece 모델을 만들어 보세요. BERT에 사용되는 주요 특수문자가 vocab에 포함되어야 합니다. (시간이 부족하다면 클라우드에 저장된 sentencepiece 모델을 사용하세요.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce488bd5",
      "metadata": {
        "collapsed": true,
        "id": "ce488bd5",
        "outputId": "09a6d2a1-cc2b-458c-f53d-8976dbd67a4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/bert_pretrain/data/kowiki.txt --model_prefix=ko_8000 --vocab_size=8007 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
            "  input_format: \n",
            "  model_prefix: ko_8000\n",
            "  model_type: BPE\n",
            "  vocab_size: 8007\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 999999\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  user_defined_symbols: [SEP]\n",
            "  user_defined_symbols: [CLS]\n",
            "  user_defined_symbols: [MASK]\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 1\n",
            "  bos_id: 2\n",
            "  eos_id: 3\n",
            "  pad_id: 0\n",
            "  unk_piece: [UNK]\n",
            "  bos_piece: [BOS]\n",
            "  eos_piece: [EOS]\n",
            "  pad_piece: [PAD]\n",
            "  unk_surface:  ⁇ \n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
            "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
            "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
            "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (2451287), which may slow down training.\n",
            "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
            "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
            "trainer_interface.cc(385) LOG(INFO) Loaded all 2451287 sentences\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
            "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
            "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(466) LOG(INFO) all chars count=287452241\n",
            "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
            "trainer_interface.cc(487) LOG(INFO) Alphabet size=4411\n",
            "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
            "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2450254 sentences.\n",
            "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2450254\n",
            "trainer_interface.cc(537) LOG(INFO) Done! 7050692\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1781571 min_freq=424\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=576838 size=20 all=581927 active=38577 piece=▁아\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390836 size=40 all=591445 active=48095 piece=▁유\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=297873 size=60 all=601378 active=58028 piece=에는\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244712 size=80 all=609974 active=66624 piece=▁성\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=194372 size=100 all=616449 active=73099 piece=까지\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193674 min_freq=462\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=176838 size=120 all=625299 active=38770 piece=▁우\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=154294 size=140 all=632274 active=45745 piece=▁파\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140625 size=160 all=639734 active=53205 piece=00\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125983 size=180 all=645481 active=58952 piece=▁요\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114855 size=200 all=649839 active=63310 piece=리아\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=114086 min_freq=457\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106760 size=220 all=657338 active=39316 piece=▁같은\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100770 size=240 all=662564 active=44542 piece=▁왕\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96037 size=260 all=670536 active=52514 piece=▁목\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=88392 size=280 all=675441 active=57419 piece=▁f\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=81678 size=300 all=681701 active=63679 piece=▁선수\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=80870 min_freq=446\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=77144 size=320 all=686930 active=39163 piece=▁때문에\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73218 size=340 all=691000 active=43233 piece=▁조선\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=68829 size=360 all=695717 active=47950 piece=▁천\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=64009 size=380 all=700839 active=53072 piece=▁196\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=60953 size=400 all=706675 active=58908 piece=▁돌\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=60762 min_freq=435\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=58542 size=420 all=711350 active=39712 piece=▁다시\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=55779 size=440 all=715377 active=43739 piece=▁K\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52528 size=460 all=721839 active=50201 piece=▁모두\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=49784 size=480 all=727356 active=55718 piece=▁히\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47637 size=500 all=733038 active=61400 piece=▁전쟁\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=47558 min_freq=423\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45919 size=520 all=738287 active=41703 piece=▁있어\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44025 size=540 all=743886 active=47302 piece=▁중심\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42378 size=560 all=748357 active=51773 piece=▁N\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=40922 size=580 all=752114 active=55530 piece=▁H\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39922 size=600 all=755142 active=58558 piece=le\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39768 min_freq=414\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38924 size=620 all=761204 active=43650 piece=▁검\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=37771 size=640 all=767376 active=49822 piece=란드\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36292 size=660 all=772837 active=55283 piece=정을\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=35134 size=680 all=778715 active=61161 piece=▁설립\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34016 size=700 all=783719 active=66165 piece=▁역사\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=34003 min_freq=400\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33144 size=720 all=787243 active=42507 piece=▁만들어\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32383 size=740 all=791538 active=46802 piece=▁시간\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31645 size=760 all=795131 active=50395 piece=▁측\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30941 size=780 all=798845 active=54109 piece=과의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30041 size=800 all=803050 active=58314 piece=도는\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30023 min_freq=392\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=29448 size=820 all=808546 active=45099 piece=▁난\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28836 size=840 all=813895 active=50448 piece=▁21\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28270 size=860 all=818654 active=55207 piece=▁찾\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27299 size=880 all=824625 active=61177 piece=되지\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26862 size=900 all=828285 active=64837 piece=하자\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=26833 min_freq=381\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26060 size=920 all=832595 active=45199 piece=부에\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25504 size=940 all=838824 active=51428 piece=수의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24726 size=960 all=843322 active=55926 piece=▁남아\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24083 size=980 all=848416 active=61020 piece=▁않는다\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23647 size=1000 all=853601 active=66205 piece=인민\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=23602 min_freq=368\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23176 size=1020 all=859529 active=48367 piece=▁마지\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22849 size=1040 all=863129 active=51967 piece=▁시리즈\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22293 size=1060 all=868678 active=57516 piece=제로\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21851 size=1080 all=873075 active=61913 piece=시의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21377 size=1100 all=878277 active=67115 piece=해야\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=21369 min_freq=355\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21043 size=1120 all=882003 active=47093 piece=▁녹\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20625 size=1140 all=886266 active=51356 piece=▁인구는\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20185 size=1160 all=889382 active=54472 piece=ac\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19762 size=1180 all=894738 active=59828 piece=인은\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19447 size=1200 all=899565 active=64655 piece=50\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=19432 min_freq=347\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19118 size=1220 all=903215 active=48471 piece=광역\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18777 size=1240 all=908007 active=53263 piece=수는\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18396 size=1260 all=913268 active=58524 piece=인민공\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18040 size=1280 all=917389 active=62645 piece=▁긴\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17659 size=1300 all=922177 active=67433 piece=▁전통\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17629 min_freq=335\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17404 size=1320 all=926354 active=50086 piece=▁망\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17093 size=1340 all=929494 active=53226 piece=단의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16931 size=1360 all=934269 active=58001 piece=▁인간\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16719 size=1380 all=939872 active=63604 piece=▁바로\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16586 size=1400 all=943447 active=67179 piece=▁탑\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16584 min_freq=327\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16331 size=1420 all=947545 active=51069 piece=▁태양\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16086 size=1440 all=952041 active=55565 piece=▁경기에서\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15873 size=1460 all=957568 active=61092 piece=▁동일\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15577 size=1480 all=962681 active=66205 piece=드를\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15352 size=1500 all=966310 active=69834 piece=▁뮤\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15328 min_freq=318\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15103 size=1520 all=968851 active=50741 piece=id\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14971 size=1540 all=972545 active=54435 piece=▁옥\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14723 size=1560 all=977306 active=59196 piece=비전\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14495 size=1580 all=982182 active=64072 piece=▁대부분의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14292 size=1600 all=985970 active=67860 piece=▁갑\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=14280 min_freq=311\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14118 size=1620 all=990010 active=53173 piece=회는\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13982 size=1640 all=993514 active=56677 piece=▁사고\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13729 size=1660 all=997224 active=60387 piece=cm\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13567 size=1680 all=1001600 active=64762 piece=▁뛰어\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13433 size=1700 all=1006574 active=69736 piece=▁더욱\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=13420 min_freq=303\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13135 size=1720 all=1011076 active=54820 piece=지역\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12940 size=1740 all=1015431 active=59175 piece=▁선언\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12821 size=1760 all=1020339 active=64083 piece=▁어려\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12640 size=1780 all=1023221 active=66965 piece=▁칭\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12505 size=1800 all=1028177 active=71921 piece=▁옛\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=12503 min_freq=295\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12409 size=1820 all=1033793 active=56931 piece=ce\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12253 size=1840 all=1037183 active=60321 piece=▁그들의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12091 size=1860 all=1041900 active=65038 piece=▁단체\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11934 size=1880 all=1045379 active=68517 piece=▁예술\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11799 size=1900 all=1048007 active=71145 piece=라의\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11799 min_freq=288\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11693 size=1920 all=1050313 active=54358 piece=▁불구하고\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11594 size=1940 all=1053703 active=57748 piece=▁분리\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11496 size=1960 all=1057998 active=62043 piece=▁사이의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11381 size=1980 all=1063418 active=67463 piece=단이\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11289 size=2000 all=1067885 active=71930 piece=im\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11277 min_freq=281\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11133 size=2020 all=1071888 active=57167 piece=▁등과\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10994 size=2040 all=1077153 active=62432 piece=법을\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10895 size=2060 all=1081334 active=66613 piece=▁괴\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10776 size=2080 all=1084885 active=70164 piece=러스\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10692 size=2100 all=1089330 active=74609 piece=▁깨\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10688 min_freq=274\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10597 size=2120 all=1091576 active=56585 piece=릭터\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10496 size=2140 all=1095180 active=60189 piece=▁확장\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10396 size=2160 all=1100510 active=65519 piece=었던\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10271 size=2180 all=1104334 active=69343 piece=▁남부\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10143 size=2200 all=1108628 active=73637 piece=▁스포츠\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10136 min_freq=268\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10057 size=2220 all=1111848 active=58430 piece=▁이외\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9957 size=2240 all=1115235 active=61817 piece=▁서식\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9848 size=2260 all=1120216 active=66798 piece=▁작용\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9732 size=2280 all=1124443 active=71025 piece=▁이야기\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9655 size=2300 all=1129106 active=75688 piece=▁성립\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9653 min_freq=261\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9570 size=2320 all=1132064 active=59284 piece=▁떨어진\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9473 size=2340 all=1135883 active=63103 piece=시를\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9352 size=2360 all=1139681 active=66901 piece=▁히로\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9289 size=2380 all=1142636 active=69856 piece=▁부정\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9219 size=2400 all=1147544 active=74764 piece=▁2020\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=9217 min_freq=256\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9137 size=2420 all=1150230 active=60032 piece=▁상징\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9028 size=2440 all=1153890 active=63692 piece=▁1950\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8965 size=2460 all=1157151 active=66953 piece=▁표시\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8889 size=2480 all=1162268 active=72070 piece=▁사용된다\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8828 size=2500 all=1165804 active=75606 piece=▁아일랜드\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8826 min_freq=251\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8738 size=2520 all=1169223 active=61615 piece=▁동생\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8694 size=2540 all=1173546 active=65938 piece=ie\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8641 size=2560 all=1176802 active=69194 piece=▁경상북도\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8570 size=2580 all=1180886 active=73278 piece=▁계승\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8489 size=2600 all=1184747 active=77139 piece=▁1985\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8487 min_freq=246\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8434 size=2620 all=1188193 active=62666 piece=력의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8345 size=2640 all=1191415 active=65888 piece=인지\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8270 size=2660 all=1196857 active=71330 piece=▁달성\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8223 size=2680 all=1200791 active=75264 piece=▁충돌\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8139 size=2700 all=1202837 active=77310 piece=ak\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=8133 min_freq=241\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8061 size=2720 all=1205959 active=63019 piece=▁2,\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=8008 size=2740 all=1208089 active=65149 piece=)\"\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7956 size=2760 all=1210710 active=67770 piece=▁둘러\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7894 size=2780 all=1213790 active=70850 piece=워드\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7817 size=2800 all=1216934 active=73994 piece=▁즐\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7817 min_freq=237\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7753 size=2820 all=1220480 active=64363 piece=석을\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7703 size=2840 all=1224543 active=68426 piece=인들은\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7656 size=2860 all=1229199 active=73082 piece=▁MBC\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7587 size=2880 all=1232852 active=76735 piece=산의\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7529 size=2900 all=1238593 active=82476 piece=생활\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7526 min_freq=232\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7474 size=2920 all=1244080 active=66850 piece=년대에\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7408 size=2940 all=1248652 active=71422 piece=▁아우\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7352 size=2960 all=1252722 active=75492 piece=▁대회에서\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7304 size=2980 all=1256113 active=78883 piece=프가\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7254 size=3000 all=1258582 active=81352 piece=가가\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=7254 min_freq=227\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7200 size=3020 all=1262333 active=66190 piece=▁출발\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7130 size=3040 all=1266484 active=70341 piece=주가\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7087 size=3060 all=1270279 active=74136 piece=▁경우도\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7049 size=3080 all=1274031 active=77887 piece=▁전개\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7000 size=3100 all=1277747 active=81603 piece=대전\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6999 min_freq=222\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6941 size=3120 all=1279813 active=65635 piece=ell\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6902 size=3140 all=1283065 active=68887 piece=트라\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6853 size=3160 all=1286899 active=72721 piece=음악\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6817 size=3180 all=1290334 active=76156 piece=▁성우\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6770 size=3200 all=1293860 active=79682 piece=▁구역\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6770 min_freq=219\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6707 size=3220 all=1297078 active=67788 piece=일보\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6660 size=3240 all=1301117 active=71827 piece=트에\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6617 size=3260 all=1304163 active=74873 piece=▁라이브\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6579 size=3280 all=1308287 active=78997 piece=▁이끄는\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6549 size=3300 all=1311934 active=82644 piece=▁1976\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6547 min_freq=214\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6517 size=3320 all=1314193 active=67834 piece=▁방식으로\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6480 size=3340 all=1317604 active=71245 piece=▁설치되어\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6448 size=3360 all=1321261 active=74902 piece=...\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6408 size=3380 all=1323725 active=77366 piece=초등학교는\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6385 size=3400 all=1327377 active=81018 piece=▁고구려\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6380 min_freq=211\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6352 size=3420 all=1329508 active=68387 piece=▁이스라엘\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6312 size=3440 all=1333149 active=72028 piece=▁초반\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6272 size=3460 all=1336419 active=75298 piece=싱턴\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6227 size=3480 all=1339417 active=78296 piece=오프\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6191 size=3500 all=1341801 active=80680 piece=시와\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6191 min_freq=208\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6155 size=3520 all=1344660 active=69389 piece=▁잭\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6115 size=3540 all=1347440 active=72169 piece=지면서\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6075 size=3560 all=1350874 active=75603 piece=세계\n",
            "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6049 size=3580 all=1355093 active=79822 piece=받은\n",
            "trainer_interface.cc(615) LOG(INFO) Saving model: ko_8000.model\n",
            "trainer_interface.cc(626) LOG(INFO) Saving vocabs: ko_8000.vocab\n"
          ]
        }
      ],
      "source": [
        "# voaab_size = 8000인 sentencepiece\n",
        "\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
        "prefix = 'ko_8000'\n",
        "vocab_size = 8000\n",
        "\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df252421",
      "metadata": {
        "id": "df252421",
        "outputId": "b2324928-4f06-4bc3-fcff-ed68a64fe7d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir = os.getenv('HOME')+'/aiffel'\n",
        "model_dir = os.getenv('HOME')+'/aiffel'\n",
        "\n",
        "# vocab loading\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(f\"{model_dir}/ko_8000.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5418498",
      "metadata": {
        "collapsed": true,
        "id": "e5418498",
        "outputId": "8a36ca2e-a603-4149-b7cf-3ad665b49698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다', '었다', '▁지', '▁수', '▁19', '▁가', '▁시', '▁20', '▁기', '▁전', '▁아', '▁하', '▁있다', '▁다', '▁제', '했다', '하였', '▁일', '▁한', '▁중', '▁정', '▁주', '하는', '▁것', '▁자', '▁공', '▁인', '되었다', '▁경', '▁위', '▁유', '▁보', '하고', '▁3', '▁등', '▁부', '하였다', '▁조', '하여', '▁미', '▁동', '▁선', '▁나', '으며', '▁모', '▁연', '▁영', '▁의', '▁오', '▁마', '에는', '▁발', '▁소', '한다', '▁고', '▁개', '▁201', '▁구', '▁세', '▁도', '▁상', '▁비', '▁스', '▁국', '▁서', '▁후', '▁여', '▁200', '▁때', '▁4', '▁성', '▁해', '▁관', '▁있는', '▁신', '▁프', '▁대한', '부터', '▁5', '▁방', '▁또', '지만', '▁(', '▁역', '되어', '▁않', '▁만', '▁\"', '▁장', '▁바', '까지', '▁무', '▁남', '▁통', '▁현', '▁교', '▁같', '에게', '▁내', '학교', '▁문', '▁출', '▁거', '▁포', '▁결', '된다', '▁《', '적인', '이라', '▁6', '▁우', '적으로', '▁불', '▁원', '▁최', '▁10', '▁진', '▁생', '▁작', '▁어', '▁당', '국의', '▁노', '▁강', '▁알', '▁반', '▁7', '▁8', '▁계', '▁따', '▁파', '▁분', '▁없', '▁받', '▁말', '면서', '들이', 'or', '되었', '▁사용', \"▁'\", '▁두', '▁실', '에서는', '▁리', '들은', '▁명', '▁함', '▁단', '▁이후', '00', '하기', '▁예', '년에', '),', '▁한다', '▁안', '하게', '▁재', '▁9', '▁일본', '▁차', '▁설', '▁배', '▁에', 'er', '▁199', '▁다음', '았다', '▁특', '▁요', '였다', '▁18', '▁데', '▁종', '으나', '▁초', '▁군', '▁로', '▁독', '▁12', '()', 'la', '▁추', '▁김', '▁많', '▁그는', '▁~', '▁대한민', '▁되었다', '리아', '▁카', '▁활', '▁감', '▁라', '이며', '는데', '▁입', 'on', '▁건', '스트', '▁타', '▁다른', '▁형', '▁더', '▁지역', '▁S', '▁참', '▁운', '▁매', '▁같은', '▁태', '▁및', '▁그리', '▁양', '▁위해', '▁평', '▁음', '▁11', 'in', '▁행', '▁산', '주의', '▁적', '▁본', '▁)', '들어', '▁미국', '▁열', '▁과', '▁왕', '들의', '▁올', '▁승', '▁때문', '▁북', '▁크', '▁시작', '▁번', '기도', 'an', '▁프로', '▁코', '들을', '▁피', '▁호', '▁총', '레이', '▁C', '라는', '▁목', '되는', '▁화', '▁그러', '▁회', '▁베', '▁따라', '▁임', '▁각', '▁198', '▁또한', '하며', '▁변', '▁사람', '▁함께', '▁A', '▁표', '▁외', '▁속', '▁처', '▁f', '라고', '▁경우', '▁물', '▁달', '▁M', '이나', '▁위치', '▁현재', '▁약', '시아', '▁자신', '▁민', '▁직', '▁이름', 'mu', '하지', '▁새', '▁간', '▁학', '▁선수', '▁그의', '▁게', '▁F', '19', '▁드', '하다', '▁집', '▁기록', '▁영화', '▁당시', '되고', '▁한국', '▁for', '에도', '▁저', '▁항', '▁가장', '▁하나', '▁세계', '▁때문에', 'en', '▁사이', '기에', '▁그러나', 'mula', '▁존', '기를', '▁메', '▁점', '▁체', 'ar', '▁박', '▁뒤', '▁것을', '▁확', '▁15', '▁또는', '▁formula', '▁했다', '▁조선', '▁있으며', '스의', '▁것으로', '▁197', '▁된다', '▁국가', '▁있었다', '이라고', '▁의해', '▁T', '▁것이', '▁레', '▁16', '▁복', '▁청', '▁축', '▁토', '▁못', '▁투', '▁천', '▁광', '리는', '▁필', '지는', '▁그리고', '▁브', '일에', '▁통해', '▁활동', '졌다', '▁법', '▁합', '▁프랑', '▁철', '▁17', '▁야', 'al', '▁판', '등학교', '▁196', '▁근', '▁창', '▁증', '▁편', '▁삼', '했고', '▁디', '▁금', '▁B', '▁치', '▁살', 'ti', '▁많은', '▁것이다', '▁언', '리를', '▁들어', '▁첫', '이라는', '▁돌', '▁버', '▁심', '▁높', '▁대한민국', '▁아니', '▁다음과', '▁‘', '하였고', '▁쓰', '(,', '▁D', '랜드', '▁하는', '▁뜻', '하면서', '▁경기', '스는', '20', '▁충', '▁다시', '▁황', 'es', '▁플', '는다', '▁서울', '▁포함', '▁대해', '▁있었', '▁가지', '▁할', '이었다', '▁194', '▁I', '▁P', '▁여러', '▁애', '▁완', '▁네', '▁트', '▁K', '▁대한민국의', '대로', '▁14', '▁되', '사를', '▁병', '▁페', '로서', 'km', '▁백', '▁독일', '이고', '▁큰', '상을', '▁죽', '개의', '이는', '▁하였다', '되었고', '▁모두', '▁루', '▁패', '▁용', '▁연구', '보다', '▁순', '▁팀', '▁도시', '▁클', '▁권', '▁13', '▁사회', '▁195', '장을', '▁채', '위를', '▁은', '▁모든', '▁L', '▁히', '▁취', '▁처음', '▁알려', '성을', '니아', 'he', '지를', '▁규', '▁이루', '▁친', '▁시즌', '▁일부', '▁중국', '이었', '▁인구', '▁등이', '▁제작', '라이', '▁대표', '▁전쟁', '▁등의', '대의', '▁이용', 'ic', '회의', '▁감독', '▁정부', '▁식', '▁축구', '▁손', '리에', '자가', 'is', '▁아이', '▁위한', '▁존재', '년부터', '▁끝', '▁구성', '▁있어', '도의', '▁R', '부분', '했으며', '▁허', '▁정치', '▁즉', '▁이상', '도로', '▁영국', '▁G', '▁프랑스', '나라', '▁-', '력을', '▁이어', '▁테', '거나', '▁것은', '▁중심', '이자', '년에는', '▁일반', '▁문제', '▁자신의', '▁길', '▁같다', '▁맡', '▁나타', '시에', '▁월', '▁러', '▁가능', '▁온', '▁작품', '▁앞', '▁논', '▁뉴', '▁환', '▁N', '000', '▁게임', '기의', '▁개발', '▁걸', '▁맞', '▁리그', '▁곳', '▁이러', '▁동안', '▁갖', '▁193', '▁발견', '▁이러한', '▁기준', '▁캐', '▁접', '▁준', '리스', '▁H', '▁별', '▁새로', '▁하지만', '▁U', '▁주장', '▁30', '▁등을', '▁이는', 'at', '년대', '▁면', '▁위치한', '▁발표', '▁향', '▁E', '▁밀', '통령', '하면', '▁국제', 'le', '▁되어', '되었으며', '▁〈', '▁방송', '도록', '로부터', '▁농', '들과', '위원', '났다', '▁협', '(1', '▁지방', '시키', '▁볼', '로는', '▁잘', 're', '자의', '▁검', '▁의미', '기는', '▁2010', '▁섬', '▁이를', '▁대부분', '서는', '군의', 'it', '▁사건', '▁전투', 'ou', '리가', '기가', '다는', '▁공격', '에서도', '▁하고', '▁번째', '란드', '▁밝', '▁담', '월에', '▁폭', '사가', '▁육', '▁귀', '▁대회', '리카', '하였으며', '▁문화', '다고', '▁홍', '▁음악', '▁영향', '▁대학', '▁대통령', '▁막', '어진', '정을', '▁얻', '이트', '리그', '세기', '했던', '▁W', '년까지', '스를', '▁최초', '명의', '쪽으로', '▁참여', '▁미국의', '▁프로그', '▁석', '▁키', '▁극', '성이', '들에게', '▁설립', '▁날', '▁우승', '▁진행', '자를', '▁설치', '인의', '▁줄', '▁결과', '▁이에', '자는', '▁효', '▁특히', '▁앨', '와의', '도를', '▁발생', '10', '▁관계', '▁관련', '▁역사', '▁<', '▁일어', 'el', '▁곡', '▁배우', '▁된', '림픽', '▁2008', '▁책', '▁그녀', '디오', '▁절', '▁“', '▁봉', '▁25', '▁유럽', '▁사망', '▁발매', '리의', '▁만들어', '▁다양', '자로', '대를', '렸다', '가지', '▁암', '▁같이', '▁러시아', '▁이전', '▁생각', '이스', '▁독립', '▁형태', '▁부분', '(19', '▁갈', '므로', '요한', '▁태어', '▁시간', 'om', '▁사실', '▁폐', '▁글', '▁인해', '▁교육', '스터', '▁블', '▁헤', 'st', '▁주로', 'ol', '▁새로운', '▁소속', '▁질', '▁자유', '▁이것', '▁받아', '▁경제', '▁측', '▁2007', '▁좋', '▁수도', '▁지원', '▁2011', '▁받았다', '▁이탈', '▁있고', '스가', '▁2009', '르는', '▁기술', '▁올림픽', '라마', '▁2012', '▁중앙', '식을', '▁와', '▁계속', '과의', '▁침', '▁출신', '화국', '▁프로그램', '▁엔', '▁머', '▁t', '▁등장', '▁송', '번째', '▁지정', '▁잡', '▁로마', '▁커', '▁상태', '▁참가', '▁범', '▁구조', '▁있던', '도는', '▁누', '-1', 'ch', '▁192', '▁데뷔', '▁탄', '▁벌', '▁골', '학자', '▁2014', '▁결정', '▁매우', '▁성공', '다가', '사의', '▁선거', '▁탈', '부의', '▁운영', '▁난', '리즈', '인이', '이지', '대학교', 'am', '▁않았다', '프트', '▁아버', '▁혼', '으로서', '▁넘', '▁아들', '이션', '▁가지고', '르크', '▁J', '▁이탈리아', '▁2013', '전을', '▁21', ',000', '▁니', '▁타이', '드는', '▁몇', '비아', '▁일본의', '▁고려', '▁윤', '기로', '▁돌아', '▁없다', '명을', '▁급', '▁아니라', '▁슈', '하기도', '▁O', '▁들', '▁찾', '대에', '처럼', 'tion', '▁터', '에서의', 'ing', '▁차지', '해서', '▁결국', '▁만들', '장이', '▁100', '도가', '했지만', '▁24', '▁생산', '▁없는', '왔다', '▁한편', '되지', '▁운동', '▁악', '▁싱', '▁2015', 'us', 'et', '▁해당', '▁최고', '▁행정', '▁떨', '▁0', '▁조직', 'il', '▁2006', '▁출연', '남도', 'ro', '▁않고', '우스', '하자', '운데', '▁아래', '▁22', '▁23', 'FA', '▁케', '사는', '▁앨범', '▁전체', '▁계획', '권을', '▁업', '▁좌', '▁많이', '간의', '▁이유', '▁내용', '▁2016', '▁2018', '부에', '▁쿠', '원을', '냈다', '▁가리', '명이', '▁라이', '▁폴', '화를', '▁음반', '스타', '▁여성', '▁둘', '▁유명', '▁따르', '군은', '수를', '▁방법', '선의', '일부터', '수의', '▁힘', '\")', '▁가운데', '▁주요', '스크', '스템', '초등학교', '으로써', '원의', '▁떠', '▁하여', '▁후보', '▁빠', 'of', '▁팔', '▁대학교', '▁역할', '▁제공', '▁데이', '▁남아', 'as', '▁국민', '음을', '▁스타', '장으로', '어로', '장은', '▁2017', '▁다양한', '▁묘', '▁월드', '▁28', '진다', '▁몸', '▁26', '사로', '지의', '나는', '▁이루어', '▁않는다', '▁거의', '▁콘', '였던', '▁표현', '대학', '▁각각', '▁컴', '자인', '▁드라마', '하던', '▁인터', '▁잠', '리고', '민주', '▁27', '▁모습', '했으나', '▁헌', '이의', '인민', '인다', '지가', '▁스페', '운동', '▁수상', '(\"', '전에', '스와', '수가', '▁연결', '▁칼', '머니', 'ra', 'BS', '▁졸', '하려', '▁붙', 'de', '▁191', '▁마지', '됐다', '▁이름을', '▁이미', '으로는', '▁V', '▁격', '▁공식', '▁190', '_1', '▁경상', '▁,', '문화', '▁출전', '원이', '▁알려져', '▁얼', '▁임명', '▁필요', '▁2005', '▁시리즈', '스로', '성의', '▁따라서', '▁열린', '가는', '▁높은', '으로부터', '대표', '위원회', '점을', '▁작은', '▁마이', '▁몰', '력이', '▁마지막', '▁지구', '▁영향을', '▁시스템', '▁뛰', '제로', '▁인정', '▁공개', '▁직접', '▁과정', '▁정보', '▁능', '시켰', '지에', '▁2000', '대한', '▁빌', '▁따르면', '▁of', '었고', ').', '어를', '▁대신', '▁기원', '▁이야', '시의', '상의', '200', '▁보고', '이가', '▁2004', '▁개최', '지고', '▁진출', '▁추가', '▁대전', '▁기능', '북도', '▁항공', '▁센', '▁자연', '▁활약', '있다', '제를', '▁않은', '해야', '▁엘', '▁풍', '▁그리스', '▁평가', '》(', '군이', '▁c', '▁민주', '▁시대', '▁되는', '▁쇼', '▁29', '▁그룹', '▁정도', '▁있는데', '▁상황', '▁홈', '▁정의', '▁오스트', '▁녹', '▁생활', '▁유지', '▁브라', '▁눈', '▁바이', '▁그가', '갔다', '▁건물', '▁졸업', '▁말한다', '이드', '선을', '사에', '▁후에', 'ed', '▁변경', '▁비롯', '▁먹', '▁인도', '▁인구는', '▁철도', '▁서로', '▁노래', '▁역시', '수로', '▁발전', '▁크게', '▁퍼', '▁보여', '▁핵', '위에', '▁아르', '▁관한', '▁제국', '▁지금', '▁않았', '▁프리', '▁압', '▁중요한', 'ac', '르트', '▁여자', '▁레이', '▁응', '인을', '▁결혼', '전에서', '들에', '▁건설', '적은', '▁뿐', '▁인물', '▁거쳐', '▁울', '▁그것', '세의', '지로', '▁학교', '▁뜻은', '인은', '▁사업', '▁하였', 'ri', '▁부산', '부를', '일까지', '▁분류', '▁m', '원은', '▁낮', '계를', '▁[', '▁FC', '▁관리', '니다', '치는', '▁나라', '▁2019', '어는', '50', '식으로', '▁경기도', '▁상대', '▁혹', '▁이론', '특별', '▁일으', '▁공동', '킨다', '▁반대', '▁플레이', '▁요구', '▁가진', '▁형성', '▁희', '▁딸', '문을', '전히', '▁티', '광역', '임을', '▁밴', '▁만든', '▁원래', '▁연합', '원으로', '치를', '▁놓', '트로', '부는', '▁모델', '▁기업', '되면서', '▁31', '▁중화', '▁시절', '▁2002', '▁않는', '레스', '수는', '른다', '▁동시에', '▁색', '▁군사', '제가', '▁이르', '▁사이에', '▁느', '리오', '▁실시', '▁시작했다', '▁지하', '▁기본', '▁워', '운드', '군을', '하였으나', '▁겸', '장의', '인민공', 'ent', '▁싱글', '▁2003', '대는', '▁어떤', '상이', '▁지역의', '▁비판', '▁승강', '▁아닌', '이었던', '▁제외', '▁노동', '▁:', '역이다', '대가', '프리카', '회를', '▁계약', '▁긴', '▁잉', '제의', '▁역할을', '▁운행', '정한', '▁현대', '▁학생', '하는데', '상으로', '▁자동', '▁50', '▁싸', '▁민족', '▁명칭', '▁지도', '▁소련', '▁야구', '이기도', '▁189', '▁전통', '▁처음으로', '▁여기', '주는', '▁최초의', 'and', '장에', '▁전라', '체의', '명은', '▁콜', '▁판매', '나다', '▁보통', '▁공간', '▁휴', '▁영화이다', '스탄', '▁이동', '▁이때', '▁망', '되며', '린다', '있는', '▁특징', '▁수행', '▁의한', '▁있을', '▁나오', '12', '▁「', '▁기준으로', '이를', '18', '▁기반', '▁기간', '▁넓', '▁도입', '▁달리', '▁되고', '단의', '▁그래', '▁초기', '▁특별', '자와', '▁견', '▁교황', '호로', '▁비슷', '▁전문', '▁옮', '▁도쿄', '소를', 'un', '키는', '▁서비', '▁푸', '▁개인', '카이', '▁있다고', '▁인간', '과는', '▁하며', '▁마을', '▁위하여', '적을', '▁기존', '▁종교', 'ig', '연구', 'BC', '.5', '격을', '서를', '성은', '▁다이', '▁퇴', '관은', '▁웨', '시마', '▁바로', '장에서', '▁개의', '▁혹은', ')·', '▁엄', 'ur', '▁지나', '▁혁', '호를', '체를', '15', '▁역임', '독교', '▁궁', '▁월드컵', '중에', '▁애니', '▁슬', '▁벨', '▁탑', '가와', '물을', '▁과학', '▁모양', '▁훈', '▁빈', '시간', '▁s', '▁출시', 'os', '▁1990', '▁목록', '▁기타', 'um', '▁정책', '▁어머니', '.\"', '▁사랑', '▁유일', '▁태양', '고등학교', '혔다', '동안', '▁지배', '무를', '트를', '명이다', '퓨터', '▁이끌', '▁아버지', '▁국내', '30', '▁일반적으로', '드로', '했는데', '드의', '▁전국', '▁될', '텔레', '▁경기에서', '하지만', '▁연방', '▁경찰', '▁기관', '광역시', '▁과거', 'od', '리로', '▁언어', '▁소설', '▁지휘', '▁주변', '16', '▁공연', '▁맥', '리와', 'ation', '도에', '화가', '▁동일', '▁기원전', '기관', 'ot', '▁샤', '▁p', '▁중심으로', '트의', '▁부르', '마다', '피언', '학과', '방송', '▁이적', '쪽에', '시켰다', '▁40', '▁산업', 'ad', '▁2001', '드를', '▁이것은', '▁보내', '▁그를', '▁받은', '스코', '▁적용', '▁탐', '▁가수', '▁비교', '▁FI', '의를', '▁흔', '구의', '▁먼', '군에', '▁1,', '▁곧', '호는', '▁획', '▁뮤', '메이션', '▁풀', '위로', '▁속하는', '경기', '▁받았', '▁추정', '▁밖', '▁작업', '▁지정되었다', '▁있다는', '▁확인', '▁분야', '▁클럽', '▁충청', '▁가까', '쳤다', '▁담당', '▁떨어', 'id', 'ter', '▁선정', '▁받고', '▁방영', '▁the', '▁찾아', '▁건축', '류의', '▁컴퓨터', '▁없이', '동에', '▁되었고', '체가', '리지', 've', '▁노선', '▁숙', '▁착', '▁국가대표', '▁옥', '당의', '▁방향', '▁챔', '▁때문이다', '▁기독교', '년의', '▁규모', '▁열차', '▁작곡', '▁피해', '어가', '▁자기', '트는', '▁통합', '▁득', '사와', '▁환경', '-2', '▁지지', '비전', '▁낙', '▁염', '▁차량', '▁증가', '▁배우이다', '14', '정에', '단을', '세는', '야마', '▁어느', '▁두고', '▁실패', '스에', 'ow', '▁설명', '하거나', 'op', '▁가족', '▁대부분의', '▁그녀는', '글랜드', '어의', '60', '조의', '크로', '▁우주', '▁멤', '인으로', '▁개념', '▁넣', '르게', '▁너', '▁기념', '▁늘', '▁올라', '▁활동을', '종의', '▁태어났다', '▁갑', '▁뉴욕', '▁이런', '11', '▁윌', '▁상당', '▁챔피언', '▁오늘', '부가', '▁최대', '▁자신이', '▁이와', '중학교', '나무', '▁수록', '▁이상의', '▁보호', '제는', 'ec', '.2', '회는', '▁캐나다', '인이다', '동을', '▁있지만', '시켜', '▁애니메이션', '▁걸쳐', '학적', '이로', '▁이름은', '▁서울특별', '▁TV', '트남', '▁1980', '▁FIFA', '일에는', '▁데이터', '▁사상', '면에', '▁사고', '▁(19', '▁하나이다', '▁어린', '원에', '▁대체', '교육', '▁다음을', '계의', '▁없었다', '▁1999', '관을', '▁있어서', '렇게', '움을', '▁수는', '▁등에', '으면', '▁대하여', '적이', 'cm', '▁멤버', '▁·', 'em', '세가', '▁스페인', '겼다', '▁1998', '식의', '▁최초로', '▁예정', '▁최근', '스카', '이란', '▁등으로', '구에', '▁있었던', '치가', '▁하다', '13', '▁뛰어', '메리카', '▁흐', '▁나가', '▁획득', '▁위원', '했다고', '성과', '일을', '17', '기와', '▁땅', 'ag', '▁선출', '▁자료', '80', '심을', '자들은', '.)', 'ir', '▁더욱', '▁불가', '▁공화국', '드리', '▁잉글랜드', '식은', '▁윈', '▁텔레', '▁개봉', '▁선택', '▁최종', '▁내려', '단은', '▁구간', '_2', '식이', '성에', '했을', '▁지역에', '세를', '지역', '▁베이', '▁인천', 'oc', '▁맺', '어서', '▁부족', '▁흑', '▁솔', '▁잔', 'ia', '▁조사', '▁조건', '인민공화국', '▁보인다', '▁시민', '널리', '▁평균', '▁d', '▁정당', '▁선언', '학을', '트리', '40', '▁물리', '▁변화', '▁앤', '▁b', '▁런', '▁조지', '시키는', '되었으나', '.0', '▁연주', '▁자리', '▁명령', 'th', '▁대구', '▁선발', '프로', '▁어려', '▁흥', '▁1997', '한다는', '▁공산', '덜란드', 'ver', '개월', '》,', '시대', '▁동물', '▁연속', '▁에너', '▁도로', '▁서울특별시', '난다', '렀다', '▁결승', '▁가리킨다', '▁점령', '▁칭', '정부', '▁있게', '함으로써', '▁이름으로', '소가', '▁불구', '▁효과', '라도', '만을', '▁작가', '▁1995', '▁반응', '▁전기', '호선', '▁표준', '명으로', '▁Y', '▁움', '▁이들은', '▁옛', '▁관할', '포츠', '▁잃', '년간', '▁거주', '▁좋은', '▁국립', '▁X', '자들이', '하도록', '▁예를', '▁헌법', '내는', '만이', '▁언급', '▁몽', '▁교통', '상에', '이지만', 'ce', '▁이유로', '▁1996', '회에서', '▁팬', '▁이해', '▁아시아', '▁선수이다', '▁컨', '구를', '▁현재는', '이에', '▁고대', '▁유사', '▁후반', '▁덕', '▁마르', '▁덴', '쪽으로는', '▁국가대표팀', '▁그들의', '▁승리', '상은', '▁전에', ']]', '▁징', '▁네덜란드', '웠다', '▁익', '비를', '▁액', '▁소유', '▁주의', '역을', '▁출판', '▁베트남', '▁왜', '인데', '▁전자', '▁조약', '▁단체', '▁목표', '▁메이', '▁정규', '▁폴란드', '스턴', '▁사무', '레일', '▁겨', '▁km', '▁북한', '었으며', '진을', '▁것이라고', 'nd', 'mm', '▁휘', '학의', '모토', '▁면적은', '▁예술', '으므로', '▁공립', '일랜드', '▁명의', '▁세포', '▁지속', '▁이곳', '▁입단', '▁하나의', '▁맡았다', '▁주민', '주에', '▁188', '과에', '▁현재의', '▁1994', '랐다', '▁시설', '▁구분', '라의', '▁모습을', '월에는', '▁쌍', '▁깊', '▁밤', '▁이듬', '▁밝혔다', '▁국회의', '▁하지', '▁버전', '▁정신', '▁영국의', '정이', '▁하나로', '▁1970', '민주주의', '▁정확', '▁드러', '▁우리', '▁불구하고', '▁돈', '▁사용하는', '▁대중', '▁아직', '베이', '되기', '▁지상', '▁말했다', '▁끌', '▁실험', '▁소프트', '▁실제', '▁오늘날', '▁소비', 'ay', '▁강화', '지지', '▁그런', '▁1992', '▁분리', '▁쓴', '70', '계에', '▁씨', '라스', '지방', '자리', '..', '▁포르', '▁제한', '▁유래', '사카', '▁[[', '▁그림', '원에서', '▁세워', '▁거두', '▁이듬해', '▁육군', '▁사이의', '나이', '▁믿', '단에', '▁크리스', '스키', '관이', '치고', '주시', '▁핀', '직을', '지와', '▁페르', '금을', '▁가문', '▁웹', '메달', '소의', '버스', '▁알려진', '단이', '▁은퇴', '▁불리', '▁언론', '시코', '▁고등학교', '회에', '▁통과', '부르크', '거리', '월부터', '스테', '▁비해', '▁찬', 'The', '▁분석', '가의', '골을', '▁위에', '▁널리', 'im', '▁남자', '▁포함한', '회가', '형을', '▁그들은', '▁의미한다', '들로', '▁제품', '▁물질', '톨릭', '▁방문', '베르', '▁나온', '▁발달', '▁헬', '▁칸', '▁펼', '▁움직', '▁회사', '▁등과', '장과', '정의', '▁중에', 'ul', '바이', \")'\", '▁경험', '로의', '▁1991', '철도', '▁차례', '▁이사', '▁결합', '스티', '▁못했다', '▁완전히', '▁제조', '체는', '▁홀', '법을', '▁비행', '▁설계', '▁크로', '▁바탕', '불어', '하의', '▁자치', '▁친구', '▁프로젝', '르고', '▁것에', '드가', '루스', '이어', '▁여겨', '▁1993', '-0', '▁전해', '▁꽃', '▁괴', '▁사람이', '▁연기', '▁요청', '▁갖고', '▁부여', 'te', '▁배치', '▁더불어', '들도', '아의', '▁통일', '▁멸', '▁자주', '▁때까지', '▁소재', '당은', '▁1960', '▁마음', '▁스웨', '러스', '선에', '▁정도로', '점이', '▁알렉', '▁독일의', '사업', '▁수준', '▁프랑스의', '▁비롯한', '▁주연', '일본', '▁빛', '시는', '▁런던', 'EFA', '▁브라질', '산을', '▁맨', '▁멀', '▁깨', '소는', '▁제안', '▁나머', '▁오른', '▁중요', '▁그대로', '찬가지', 'TV', '▁영토', '▁목적으로', '▁마찬가지', '▁1988', '▁초등학교', '▁오래', '▁파괴', '▁요시', '로를', '▁쉽', '▁바뀌', '릭터', '▁유전', '트가', '▁아프리카', '▁감독의', '▁스스로', '▁이로', '위가', '부로', '▁주인', '경을', '▁텔레비전', '스토', '▁않아', '웨어', '▁인터넷', '▁만나', '▁유명한', '▁60', '위는', '▁확장', '지에서', '기술', '우는', '▁있도록', '▁외국', '▁패배', '관으로', '》()', '▁성장', '.3', '▁이들', '▁식민', '인과', '정은', '▁추진', '▁밝혀', '선거', '가를', '▁오스트리아', '었던', '▁의하여', '▁많다', '▁치료', 'ap', '\",', '▁스웨덴', '문에', '위의', '▁존재한다', '▁사라', '선이', '▁달러', '▁UEFA', '▁하이', '물의', '▁지난', '▁우승을', '▁아마', '주를', '▁남부', '▁기여', '▁사람들이', '▁칠', '▁가르', '받았다', '▁복귀', '되기도', '일의', '지게', '감을', '르의', '▁불교', '아가', '▁경우가', '▁경쟁', '▁1989', '종이다', '▁혈', '없이', '▁스포츠', '영화', '▁과정에서', '신을', '▁강력', '▁근처', '▁법률', '▁습', '▁몬', '▁짧', '▁되었', '▁먼저', '▁일어나', '▁하계', '▁캐릭터', '▁공항', '▁해석', '▁왕국', '회사', '▁있으나', '▁이외', '▁초대', '라인', '멕시코', 'NA', '▁주도', '▁것이었다', '많은', '▁영어', '▁접근', '48', '▁수용', '▁가톨릭', '▁광주', '규모', '▁폐지', '력은', '▁집합', '▁일제', '▁밴드', '▁서식', '▁들어가', '▁관측', '▁불리는', '▁품', '▁선수로', '이기', '카와', '하라', '스에서', '레일리아', '▁KBS', '▁가능하다', '▁단순', '군과', '도와', '▁알고', '조는', '▁그녀의', '▁잇', '▁작용', '▁제작된', '▁개혁', '▁특정', '▁촉', 'li', '구는', '행을', '▁스위', '정보', '▁않다', '25', '오는', '▁가져', '디어', '▁구성되어', '▁기초', '형의', '▁필리', '화국의', '▁이야기', '▁도착', '▁십', '▁인민', '▁노르', '▁때는', '▁결성', '하다가', '▁여기서', '과학', '▁교수', '하이', '▁제국의', '소년', '▁비슷한', '자이다', 'og', '▁70', '주로', '▁여객', '▁성립', '▁술', '▁St', '졌고', '▁/', '▁강원', '▁자동차', '▁조각', '량이', '간다', '▁186', '▁해결', '▁궤', '▁조선민주주의', '▁선수권', '▁수많은', '수한', '전의', '▁거부', '▁록', '▁떨어진', '▁스테', '하였는데', '▁아카', '크의', '▁해안', '▁경영', '_3', '▁앙', '웨이', '▁사용한다', '▁디자인', '▁파견', '▁기록했다', '▁상대로', '▁삼성', '▁반란', '속도로', '고를', '▁아무', '시를', '▁지역에서', '▁라디오', '▁바꾸', '▁지냈다', '했다는', '▁행동', '선은', '▁상호', '▁생성', '▁의하면', '▁보다', 'ab', '▁동맹', '▁『', '▁다르', '▁발행', '▁겪', '▁중간', '▁아시', '▁히로', '▁여름', '▁미국에서', '▁북쪽', '▁그린', '▁이루어진', '▁살아', '▁놀', '전이', '▁외교', '어나', '▁187', '▁의원', '▁대표적인', '▁을', '▁남쪽', '▁황제', '▁프레', '90', '▁제시', '▁부정', '▁당선', '센터', '구가', '하기로', '▁나무', '▁대해서', '▁승강장', '▁훈련', '▁슈퍼', '▁지하철', '함을', '▁공작', '▁임시', '▁분포', '자에', '▁타이틀', '▁다만', '▁시행', '▁만에', '▁2020', '촬영', '▁논란', '량을', '▁것과', '▁마찬가지로', '▁오스트레일리아', '▁1945', '(200', '▁형태로', '▁이렇게', '▁투표', '▁근거', '▁위해서', '▁엔진', '미터', '▁in', '부에서', '▁방어', '▁이주', '▁상징', '▁처리', '트워', '관의', 'ard', '▁흡', '▁캠', '▁중에서', '▁빅', '와는', '인트', '▁번역', '▁위험', '아시아', '▁계열', '▁대응', '▁시작하였다', '낸다', '▁교회', '▁일을', '▁1950', '▁끝에', '(20', '▁윌리', '만의', '▁스트', '전은', '왕의', '▁1986', '나가', '▁출생', '▁등록', '▁유니', '았고', '▁부른다', '▁멕시코', '▁1987', '▁디지', '▁년', '지션', '▁표시', '▁벽', '▁개통', '▁야마', '였고', '한다고', '▁시도', '▁n', '▁재판', '업을', '▁제주', '교회', '면을', '시즌', '으면서', '▁붕', '▁꾸', '▁체포', '성으로', '▁준비', '▁사용된다', '▁값', '평양', '▁곳에', '▁즉위', '▁몇몇', '▁있었고', '▁뇌', '▁통치', '조선', '▁있기', '크라', '▁규정', '▁?', '▁삶', '화된', '.1', '.4', '사에서', '▁한국의', '▁아일랜드', '사회', '▁이름이', '▁숨', '21', '▁적이', '▁사용할', '▁곤', '점으로', '▁좋아', '▁시험', '조를', '년을', '요일', '▁회의', '어졌다', '23', '▁활용', '▁쌓', '▁체결', '▁동생', '▁바다', '▁보유', '리에서', '문화재', '▁중단', '24', '▁따른', '▁캘', '디아', '▁제기', '▁대해서는', '▁벤', '화의', '오카', '.6', '86', 'ut', '▁그렇', '학생', 'ie', '▁있지', '▁낳', '대와', '▁본래', '▁1984', '▁보존', '▁다카', '▁미사', '▁안정', '▁다루', '▁종합', '▁나타나', '인들이', '▁조성', '▁가정', '▁직후', '▁귀족', '▁80', '▁유로', '▁경상북도', '▁윈도', '나의', '연합', '▁카를', '▁관련된', '▁종류', '▁밑', '물이', '▁나머지', '부와', '국이', '당이', '해졌다', '▁얼마', '▁시기', '▁지적', '량은', '▁표기', '▁경상남도', '▁계승', '수이다', 'ine', '가이다', '도시', '▁살해', '▁주인공', '▁촬영', 'ht', '▁것도', '▁오브', '▁못하고', '35', '▁완성', '.7', '▁보수', '▁센터', '가로', '습니다', '▁하루', '▁1985', '▁팀의', '▁문학', '▁엘리', '▁서비스', '▁and', '▁못한', '▁떠나', '▁혁명', '였으며', '▁옮겨', '▁사진', '▁내각', '리포', '로가', '니는', '▁동쪽', '▁오후', 'ion', '▁수비', '력의', '나라의', '트에서', '러운', '▁대사', '▁둔', '▁강한', '되자', 'ess', '하에', '▁천황', '▁가능한', '▁금지', '▁대륙', '▁혐', '역으로', '▁유일한', '▁이루어져', '▁위치하고', '▁싶', '인지', '시키고', '전한', '권의', '선에서', '롭게', '▁홍콩', '자들의', '소에', '내고', '▁묘사', '시킨', '▁이스', '▁취임', '안을', '소로', '주교', '▁윌리엄', '▁안에', '▁류', '▁달성', '▁왼', '체로', '려고', '▁유대', 'nt', '서로', '▁비디오', '▁젊', '▁디지털', '▁바르', '국은', '▁해외', '▁스코', '▁의견', '계가', '▁종료', '▁포르투', '티아', '學校', '▁충돌', '▁정부는', '▁르', '▁베르', '▁물론', '▁이용하여', '력으로', '▁벗', '▁스튜', '▁친일', '89', '▁각종', '▁일어난', '▁소개', '▁인근', '▁중의', '▁경우에는', '▁이집', '▁지명', 'br', 'ak', '민족', '치의', '피아', '▁전달', '▁근무', '▁사이에서', '▁필요한', '▁간의', '▁아내', '▁35', '▁위치해', '▁승격', '▁문제가', '▁오사카', '세에', '▁주목', '▁관광', '헝가', '▁억', '▁2,', '▁경기를', '▁가까운', '당을', '▁발사', '▁지역을', '월까지', '로로', '▁빨', '치로', '▁낮은', '▁읽', '▁참여하였다', '▁말을', '▁도시로', '▁32', '▁제거', '임스', '▁베네', 'ity', ')\"', '▁죄', '▁아니다', '▁나누', '▁파이', '▁관계를', '▁위의', '▁전환', '▁듣', '▁이슬', '분을', '적으로는', '조가', '92', '▁1982', '▁합류', '45', '분에', '▁만들었다', '▁남북', '▁둘러', '▁가입', '▁뒤에', '▁알렉산', '▁문제를', '량의', '▁만들어진', '▁폰', '▁썼', '단으로', '▁굴', '작용', '▁중국의', '▁미술', '▁해군', '▁서쪽', '기업', '▁빼', '▁이래', '승을', '워드', '▁명예', '고로', '▁만화', '라우', '▁에이', '▁되었으며', '▁평화', '▁후쿠', 'ph', '▁제도', '▁수가', '▁꼬', '▁출신의', '▁주는', '준다', '니스', '▁불린다', '▁건너', '▁구조를', '▁즐', '▁Ch', \"',\", '▁북부', '.(', '▁갖추', '물관', '그룹', '▁뮤직', '라고도', '▁1983', '▁바탕으로', '▁네트워', '▁미래', '▁정식', '지도', '▁그려', '▁반면', '▁계산', '▁실제로', '석을', '▁옆', '론을', '▁실행', '▁멜', '▁신라', '▁전반', '상과', '▁전시', '▁전략', '되었던', '▁펜', '▁종목', '종이', '▁최고의', '▁너무', '▁마련', '▁정치적', '▁기존의', '▁제목', '인들은', '▁만약', '만에', '일은', '문학', '▁전라남도', '메이', '▁w', '▁부문', '계는', '▁주었다', '신의', '▁오르', '▁In', '권이', '85', '▁사실을', '▁납', '프는', '▁재직', '▁MBC', '▁그해', '▁비밀', 'ist', '시가', '▁기록을', '▁저항', '산으로', '버트', '▁스토', '▁집중', '▁신문', '▁강조', '이던', 'ge', '▁The', '▁통하여', '▁사람은', '▁가는', '▁인식', '산의', '▁부모', '▁넘어', '리어', '미널', '▁맹', '▁배경', '▁생물', '▁연장', 'ber', '▁기후', '산업', '\"(', '즈의', '사이', '먼트', '상에서', '▁작성', '-3', 'ne', '생활', '관리', '▁아키', '메달을', '▁상업', '▁던', '▁사령', '이라고도', '▁정착', '▁마이크로', '점에서', '▁불러', '▁성격', '비가', '크는', '했기', '▁무렵', '▁II', '▁인한', '로스', '년대에', 'CA', '차를', '▁적극', '▁기능을', '▁끼', '▁제외한', '▁하기', '라노', '▁보여주', '▁영역', '질을', '세포', 'est', '▁허용', '▁혜', '▁객', '단체', '▁공급', '보다는', '▁아우', '졌으며', '해를', '▁올랐다', '은행', '교의', '공항', '두고', '시대의', '장이다', '로운', '▁로스', '▁내부', '▁확대', '▁셀', '▁뿐만', '에게는', '▁유형', '▁왔다', '법원', '▁대회에서', '파의', '▁이후에', '전쟁', '47', '▁최소', '▁입학', '▁사용하여', '▁사회주의', 'all', '▁일반적인', '▁나중에', '▁일이', 'ran', '▁의사', '▁오스', '▁사람을', '밖에', ')》', '계에서', '프가', '▁항상', '▁헨', '▁포지션', '22', '▁받아들', '▁정부의', '▁생활을', '▁이슬람', '▁1979', '리포니아', '▁여전히', '▁지진', '▁뒷', '▁족', '▁터키', '▁만드는', '.8', '▁부근', '적이고', '가가', '물로', '▁1981', '▁국방', '▁보면', '▁인디', '▁관찰', '▁전투에서', '르타', '▁이번', '▁제정', '▁케이', '▁그래서', '래프트', '되었는데', 'qu', '.9', '▁시대의', '▁판단', '세이', '▁출발', '▁보도', '▁동료', '▁동부', '▁연극', '▁1972', '년과', '▁원자', '▁확보', '▁핀란드', '호가', '과를', '▁위협', '▁있었지만', '개발', '기고', '▁편집', '▁채널', '▁매년', '원과', '주가', '▁사용되는', '한국', 'ers', '▁a', '▁덴마', '▁보이', '▁붕괴', '▁선의', '▁가리키는', 'ub', '▁바람', '▁쉽게', '▁포르투갈', '▁국왕', '▁뒤를', '면의', '▁등은', '시오', '다이', '▁경우도', '27', '▁정리', '사령', '▁de', '까지의', '▁채택', '로도', '서관', '▁마리아', '았으며', '▁문서', '26', '끄는', '▁젊은', '력에', '▁충분', '▁90', '▁동계', '▁교체', '▁전개', '36', '▁마사', '동차', '▁흰', '▁높이', '실을', '▁이승', '경제', '▁방식', '▁시의', '시로', '▁범죄', '▁시작되었다', '▁1973', '▁합병', '▁500', '▁세운', '▁유리', '▁남동', '대전', 'tic', '▁승리를', '▁끊', '▁모스크', '▁일종이다', 'ST', '▁남편', '▁외에도', '▁말이다', '▁공격을', '▁겨울', '▁다섯', '▁감소', 'art', '▁하자', '▁점차', '37', '▁아나', '▁장기', 'ell', '▁척', '세기에', '▁시기에', '46', '구려', '▁없어', '▁득점', '모리', '화에', '▁떨어져', 'FC', '라운드', '▁계약을', '▁자리를', '▁밖에', '력과', '비는', '▁탑재', '▁측정', '트라', '▁귀국', '당에', '드와', '▁강력한', '로서의', '87', '▁옹', '배우', '▁등에서', '▁전사', '▁의해서', '▁중학교', '▁사람들은', '▁울산', '리기', '▁사용한', '▁서부', '▁블랙', '▁k', '음악', '▁구축', '▁그곳', '의로', '▁녹음', '▁잠시', '하려는', '▁강원도', '▁민간', '▁수학', '대에서', '테인', '▁본관은', '▁얼굴', '▁수립', '▁관심을', '▁냉', '데미', '▁갖춘', '▁하게', '▁성우', '▁책임', '까지는', '▁세르', '▁설립된', '▁에드', '자치', '▁하면서', '계로', '▁당시의', '▁헝가', '문이', '▁캘리포니아', '▁안전', '▁정보를', '▁통신', '▁충청남도', '▁해체', '▁필리핀', '당한', '▁구역', '▁주장했다', '▁변호', '협회', '▁시청', '▁흘', '▁인수', '▁평양', '▁g', '95', '▁파일', '▁증거', '설을', '▁춘', '▁마쓰', '▁45', '▁신경', '▁1974', '▁185', '라엘', '일보', '붉은', '▁알아', '▁불과', '▁자체', '▁화학', '▁침공', '▁해방', '▁균', '구로', '▁만족', 'uc', '스케', '▁왔', '해의', '▁집단', '정으로', '▁구단', '▁잎', '받아', '트에', '▁레코', '되었지만', '면에서', '▁v', '▁주연으로', '▁길이', '▁예선', '▁간주', '98', '▁종종', '틀랜드', '▁사후', '▁발표했다', '▁나뉘', '▁게이', '▁소프트웨어', '다운', '▁경향', '▁요리', '▁라이브', '네시아', '▁건강', '▁델', '일러', '▁여행', '▁시내', '▁대립', '▁쇠', '용으로', '▁나폴', '우리', '▁샌', '▁기획', '민국', '개를', '사키', '▁좀', '무원', '티브', '▁이끄는', 'ice', '▁텍', '▁쪽', '▁이란', '▁개봉한', '▁쓰인다', '▁인간의', '하나', '하다고', '▁임명되었다', '약을', '레이션', '리스트', '▁에스', '▁무역', '▁편입', '▁죽은', '▁음력', '같은', '▁1976', '▁봄', '▁찰', '▁회원', '▁태평양', '었는데', '▁정상', '▁힘을', '류를', '▁컬', '▁나오는', '티나', '▁자신을', '학년', '▁짓', '▁폭발', '▁태어난', '▁때에는', '▁제임스', '▁받는', '▁방식으로', 'our', '▁엑', '▁인사', '자들을', '▁1978', '▁회복', '자에게', '▁개편', '▁비교적', '▁대형', '▁인기를', '▁수입', '▁한다는', 'se', '▁장애', '명한', '75', '▁대상', '후의', '▁설치되어', '▁속에', '▁뛰어난', '▁연맹', '하고자', '키아', '▁기계', '▁라틴', '리학', '▁사례', '▁둥', '▁공공', '▁조정', '박쥐', '▁자본', '▁수정', '▁국회의원', '적이다', '▁아름', '점에', '...', '었으나', '▁1920', '▁조선민주주의인민공화국', '▁&', '▁특수', '▁1971', '▁신설', '▁남성', '▁노력', '▁후손', '▁어떻', '다른', '▁했지만', '▁블루', '라비아', '▁메이저', '신이', '▁루이', '▁에피', '초등학교는', '음이의', '▁숲', '족의', '▁고양', '▁전차', '65', \"'(\", '데스', '32', '▁이것이', '▁저장', 'ip', '▁보이는', '▁형식', '▁위치한다', '종은', '▁사람의', '▁대규모', '화는', '▁고구려', '▁1968', '▁1963', '▁1975', '▁몽골', '▁180', '▁초등학교이다', '▁함수', '▁불가능', '▁카메', '▁피아', '▁복잡', '니어', '포르', '▁네트워크', '▁프로듀', '▁발견된다', '▁농업', 'av', '룬다', '▁이스라엘', '어져', '▁내에서', '형이', '▁e', '▁않을', '로나', '여자', '▁부터', '▁1930', '정책', '▁파리', '▁털', '동의', '▁한다고', '▁원칙', '▁문화재', '▁사실상', '료로', '▁우선', '▁초반', '▁흔히', '▁고전', '리트', '▁내용을', '▁시대에', '▁편성', '▁홈런', '▁식물', '▁본격', '소에서', '▁1940', '▁감염', '▁링', '이었고', '▁아르헨', '유럽', '차례', '43', '▁꿈', '싱턴', '▁있었으며', '▁콩', '▁발매되었다', '군으로', '▁과정을', '49', '▁조금', '▁부인', '종으로', '▁1969', '▁스튜디오', '▁외에', '국을', '▁쉬', '▁기억', '▁구성된', '▁인하여', '▁근대', 'ik', '오프', '▁나카', '▁북서', ',5', '▁트리', '▁300', '▁지역은', '구역', '▁속에서', '터테인', '▁팀은', '▁나서', '▁전혀', '셔널', '▁이전에', '▁어떻게', '미디', '▁모스크바', '제에', '▁원작', '시와', '▁참석', '료를', '품을', '▁아주', '▁도움을', '▁형태의', '▁유명하다', '▁통한', '▁윈도우', '▁있었으나', '예술', '▁열대', '▁구체', '▁아랍', '▁오리', '▁행사', '크라이나', '려는', '▁죽음', '▁잭', '지기', '분의', '▁대상으로', '아버', '▁잘못', '▁증명', 'for', '▁세이', '▁암살', '▁중화인민공화국', '▁설정', '▁사회적', '▁섭', '▁물러', '76', '▁때에', '▁하는데', '▁사용되었다', '전으로', '지면서', '28', '▁탄생', '▁본선', '▁확정', '▁플로', '드에서', '라크', '▁사는', '대회', '었지만', '▁1948', '▁Z', '▁정도의', '▁비난', '법의', '서에', '▁캄', '▁숫', '▁카운', '세계', '공업', '인들의', '▁등장하는', '어와', '▁공부', '▁했고', '▁시장', '▁창설', '년이', '▁짧은', '동음이의', '권에', '▁되면서', '어도', '▁개선', '▁박사', '▁현실', '▁철도역이다', '▁보급', '받은', '▁묻', '▁하였고', '▁기사', '▁한때', '권은', '▁뜻이', '라가', '법에', '인에', '▁', '이', '다', '.', '에', '의', '는', '로', ',', '하', '1', '을', '가', '고', '지', '서', '한', '은', '기', '0', ')', '(', '으', '2', '사', '대', '리', '시', '를', '년', '스', '도', '인', '일', '아', '자', '9', '어', '있', '라', '수', '나', '부', '그', '전', '되', '정', '국', '과', '해', '주', '었', '제', '들', '성', '장', '3', '구', '여', '상', 'e', '적', '동', '했', '5', '8', '월', '위', 'a', '와', '4', '원', '선', '게', '공', '였', '보', '6', '마', '만', '조', '트', '중', '며', 'o', '드', '7', '화', '경', '학', '미', '교', '소', '우', '유', 'r', '세', 'i', '비', '신', '오', '문', '치', '르', 'n', '명', '개', '면', '계', '역', '후', '연', '관', '등', '회', '된', '것', '진', '터', '영', '모', '용', '\"', 't', '발', '프', '군', 'l', '당', '무', '산', '러', '작', '재', '방', '민', '타', '니', '음', 'm', '데', '단', '거', 's', '레', '통', '간', '카', '분', '반', '생', '바', '안', '체', '포', '차', '행', '크', '내', '호', '현', '식', '운', '본', '노', 'u', \"'\", '남', '할', '종', '파', '물', '양', '요', '때', '출', '속', '임', '실', '야', '두', 'S', '-', '함', '형', '던', 'C', '결', '토', '강', '설', '루', '려', '표', 'A', 'c', '히', '키', '코', '각', '법', '초', '승', '독', '태', '업', '직', '건', '까', '디', '불', 'h', '래', '입', 'd', '배', '버', '력', '합', '록', '또', '점', '최', '예', '않', '번', '받', '매', '피', '같', '았', '약', '천', '른', '권', '왕', '말', '베', '메', '티', '《', '》', 'M', 'T', '심', '판', '격', '립', '알', '브', '청', '더', '금', '목', '감', 'B', '투', '추', '처', '북', '없', '항', 'f', '란', '달', '~', '평', '활', 'F', 'I', '외', '네', '열', '따', '린', '특', '저', '·', 'g', '집', '언', '편', 'P', '석', '준', '복', 'D', '테', '향', '즈', '병', '광', '류', '변', 'p', 'k', '질', '름', '령', '난', 'y', '족', '근', 'E', '랑', '급', '총', '람', '황', '송', '김', '존', '페', '축', '많', '올', '쪽', '능', '박', '술', '참', '철', '울', '론', '별', '살', 'L', '환', 'R', 'N', '온', '창', '악', '증', '품', '절', '및', 'O', '육', '료', '곡', '쿠', '백', ':', '새', '쓰', 'K', '플', '져', 'G', '순', '돌', '졌', '량', '필', '께', 'b', '확', '애', '망', '든', '머', '규', '림', '글', '패', '케', '색', '워', '팀', '범', '탈', '날', '견', '극', 'H', '쟁', '클', '련', '삼', '탄', '너', '친', '충', 'V', '랜', '째', '손', '봉', '막', '_', '책', '접', 'v', '뒤', '못', '완', '협', '퍼', '션', 'U', '/', '담', '런', '밀', '채', '츠', '길', '녀', '습', '골', '측', '블', '혼', '곳', '취', 'W', '럽', '엔', '험', '칭', '귀', '누', '겨', '암', '벌', '찰', '슬', '갈', '커', '씨', '’', '앙', '높', '웨', '슈', '‘', '첫', '뜻', '쳐', '침', '허', '즌', 'w', '죽', '효', '획', '볼', '널', '뉴', '응', '센', '났', '착', '큰', '락', '잡', '헌', '례', '희', '렸', '검', '폭', '득', '징', '움', '논', 'J', '%', '농', '섬', '걸', '캐', '홍', '럼', '틀', '풍', '느', '좌', '퇴', '택', '왔', '끝', '폴', '즉', '엘', '폐', '층', '될', '압', '줄', '픽', '헤', '덕', '윤', '싱', '맞', '먼', '념', '앞', '>', '찬', '맡', '긴', '<', '혁', '늘', '갖', '킨', '율', '써', '익', '〈', '〉', '턴', ']', '[', '릭', '잘', '램', '묘', '빈', '슨', '염', '링', '냈', '얼', '벨', '흥', '균', '억', '델', '닌', '앨', '객', '궁', '콘', '밝', '몇', '엄', '맹', '빌', '휘', '훈', '팔', '옥', '얻', '숙', '컵', '렌', '칸', '갔', '쇼', '“', '”', '텔', '칼', '됐', '므', '괴', '넘', '콜', '맥', '푸', '뷔', '좋', 'X', '몰', '붙', '덴', '핵', '찾', '혹', '낸', '흐', '둘', '빠', '떠', '켰', '템', 'x', '액', '혀', '샤', '컴', '릴', '몬', '뿐', '놓', '룹', '률', '쥐', 'z', '탑', '굴', '욕', '십', '끌', '떨', '힘', '잠', '롯', '싸', '죄', '칙', '갑', '핀', '벽', '척', '졸', '략', '춘', '풀', '몸', '먹', '눈', '맨', '켜', '잉', '끼', 'Y', '촌', '휴', '쳤', '잔', '컬', '홈', '벤', '칠', '렉', '밖', '大', '틴', '녹', '렇', '뛰', '몽', '뮤', '곤', '탁', '!', '륙', '폰', '털', '꾸', '혜', '멸', '탕', '둥', '겼', '밴', '웠', '솔', '겸', '혈', '돈', '딸', '톤', '렀', '혔', '셀', '쿄', '팅', '?', '낮', '넷', '컨', '홀', '앤', '님', '둔', '퓨', '냐', '」', '「', '윈', '텐', '랐', '겠', '욱', '롤', '톨', '탐', '떤', '헨', '꽃', '낙', '넓', '촉', '킬', '읍', '왜', '金', '셔', '콩', '렬', '붕', '山', '펜', '뇌', '랙', '옹', '덜', '슷', '흔', '융', '李', '옮', '섭', '=', '듬', '넣', '멘', '빛', '깨', '놀', '곧', '땅', '學', '킹', '닝', '룬', '헬', '팬', '딩', '쌍', '흑', '겐', '힌', '답', '켓', '챔', '롭', '룡', '값', '튜', '멤', '짓', '젤', '큐', '즘', '윌', '쉬', '쇄', '밤', '릉', '렵', '뢰', '웅', '닉', '흡', '튼', '젠', '씩', '납', '듀', '옛', '맺', '닥', '궤', '쉽', '짜', '랭', '숭', '듯', '+', '랫', '中', '文', '잃', '州', '숨', 'Z', '섯', 'j', '젝', '깊', '웃', '&', '롱', '빙', '꼬', '멜', '國', '멀', '쓴', '王', '캠', '펠', '웹', '핑', '큼', '잇', '즐', '끄', '텍', '믿', '뜨', '찍', '잎', '셰', '덤', ';', 'Q', '』', '『', '밍', '펼', '뷰', '짐', '틱', '빅', '돼', '뀌', '멕', '빨', '엽', '틸', '쇠', '짧', '삭', '곱', '子', '롬', '|', '校', '城', '첨', '폼', '東', '힐', '촬', '럴', '럭', '릿', '혐', '三', '道', 'q', '켈', '캘', '썼', '겪', '安', '곽', '낭', '랍', '딘', '옆', '밑', '켄', '氏', '쌓', '삶', '콤', '高', '흘', '人', '南', '벗', '뉘', '싶', '헝', '첩', '낳', '컷', '젊', '닐', '꿈', '냥', '빼', '옷', '랄', '天', '왼', '듣', '됨', '公', '얀', '읽', '닛', '톱', '냉', '셜', '°', '믹', '룩', '平', '寺', '엑', '픈', '事', '法', '벡', '낼', '섰', '숲', '퀴', '봄', '뒷', '엇', '띠', '書', '흰', '宗', '軍', '묵', '벼', '셋', '뤄', '튀', '븐', '붉', '잭', '쿨', '캄', '川', '元', '좀', '앗', '괄', '샌', '뿌', '義', '正', '끊', '뱅', '슴', '맛', '덮', '一', '씬', '넬', '팽', '녕', '륜', '찌', '–', '떻', '숫', '댄', '뱀', '府', '늬', '묻', '院', '長', '첼', '팩', '∼', '・', '짝', '둑', '웰', '石', '춤', 'é', '늄', '成', '쾌', '렴', '톰', '明', '쓸', '海', '셈', '탱', '뤼', '앵', '캔', '엠', '꼽', '깔', '陽', '뽑', '샘', '늦', '렐', '훨', '꺼', '밥', '닫', '西', '쫓', '光', '옌', '횡', '꼭', '君', '上', '太', '앉', '使', '郡', '팝', '德', '會', '原', '뼈', '륨', '地', '덩', '흉', '좁', '끈', '北', '겔', '武', '世', '끔', '家', '行', '部', '新', '봇', '렘', '本', '훗', '主', '꾼', '룸', 'ᆞ', '等', '깃', '룰', '뚜', '日', '都', '멍', '田', '윗', '神', '水', '韓', '덧', '춰', '펙', '랩', '門', '仁', '햄', '갤', '縣', '홋', '政', '퀘', '펴', '듭', '딕', '딜', '삽', '쿼', '꺾', '*', '龍', '生', '묶', '싼', '궐', '官', '륭', '經', '司', '탠', '퀸', '뮌', '윙', '無', '쌀', '꼴', '섞', '朴', '뉜', '겹', '里', '빗', '킴', '興', '눌', '훌', '띄', '趙', '×', '林', '긍', '첸', '슐', '펄', '洞', '忠', '녁', '밭', '셉', '깥', '돔', '相', '넌', 'а', '敎', '녔', '넥', '漢', '夫', '툰', '렛', '白', '民', '所', '永', '江', '聖', '낌', '五', '鄭', '펑', '臣', '和', '慶', '理', '빵', '돕', '馬', '깝', '史', '뿔', '張', '엉', '늑', '섹', '곰', '펀', '똑', '닷', '틈', '훼', '→', '全', 'の', '랴', '딱', '넨', '皇', '善', '컫', '前', '昌', '記', 'о', '흙', '힙', '朝', '굳', '將', '島', '帝', '툴', '갱', '定', '性', '핸', '心', '羅', '팡', '知', '尹', '化', '킥', '셸', '콰', '堂', '빚', 'и', '킷', '톡', '有', '겁', '껍', '之', '士', '풋', '小', '面', '맷', '닭', '벳', '崔', '十', '方', '겉', '京', '河', '松', '社', '쏘', '同', '뮬', '룽', '佛', '下', '時', '不', '淸', '굽', '멈', '古', '콥', '콕', '宮', '옵', '콧', 'α', '뇨', '봐', '푼', '通', '뻗', '代', '밸', '핫', '턱', '四', '信', '師', '딴', '럿', '年', '二', '떼', '꿀', '굿', '像', '렷', '탓', '퉁', '쯤', '論', '華', '죠', '後', '守', '닮', '劉', '樂', '쑤', '홉', '禮', '月', '祖', '八', '왈', '孝', '샹', '尙', '캡', '女', '굉', '줬', '權', '릎', '눅', '릇', '重', '圖', '쁜', '市', '붓', '뚫', '木', '字', 'е', 'н', '툼', '洪', '基', '칩', '黃', '싫', '낱', '物', '샬', '孫', '댐', '雲', '셨', '內', '陵', '놈', '立', '엣', '名', '業', '띤', '닿', '걷', '寧', '侯', '鎭', '듈', '吉', '낀', '合', '自', '科', '、', '廣', '驛', '헐', 'р', '삿', '判', '兵', '集', '갓', '흠', '派', '젖', '功', '殿', '柳', '順', '分', '봤', '監', '냄', '쩌', '衛', '宋', '承', '體', 'ー', '靑', '솟', '六', '春', '펌', '잊', '谷', '웬', '걱', '九', '周', '建', '議', '퐁', '景', '쉐', '※', '曹', '덟', '앱', '直', '옴', '솜', '•', '制', '돋', '곁', '美', '흩', '쥬', '쿤', '度', '톈', '둠', '슘', '者', '얄', '얇', '郞', '源', '굵', '英', '路', '륵', '긋', '쏟', '村', '얘', '福', 'á', '휩', '껴', '館', '멋', '짙', '宣', '見', '身', '左', '章', '萬', '팜', '탤', '花', '星', '貞', '꾀', '觀', '號', '렁', '#', '맵', '眞', '的', '$', '初', '節', 'с', '쯔', '밟', '濟', '꿔', '動', '治', '野', '훔', '덱', '泰', '뭉', '侍', '康', '副', '컸', '傳', '造', '챌', '캉', '왓', '뚝', '開', '根', '뤘', '外', '典', '音', '国', '壽', '떡', '令', '臺', 'к', '式', '쁘', '옐', '寶', '玉', '끓', '別', '應', '憲', '利', '햇', '{', '吳', '族', '奉', '思', '휠', 'い', '鮮', 'в', '뀐', '텀', '핍', '댓', 'ü', '后', '實', '命', '戰', '`', '씀', '語', '짱', '良', '御', 'ン', '唐', '弘', '修', '꽤', '保', '申', '촨', '잦', '맘', '횟', '智', '슭', '說', '浦', '親', 'л', '在', '作', '賢', '空', '錄', '阿', '픔', '形', '줌', '펫', '來', '塔', '省', '梁', '썬', '빔', '進', '位', '괘', '土', '敬', '앓', 'т', '老', '詩', '싯', '歌', '뒀', '벅', '밋', '工', '色', '界', '津', 'ا', '徐', '붐', '右', '엿', 'ο', '線', '參', '井', '秀', '碑', '갇', '얹', '땄', '눠', '七', '流', '延', '溪', '齋', '言', '風', '陳', '多', '깎', '딪', '護', '運', '멧', '샨', '間', '헥', '땃', '줘', '用', '팟', '如', '數', '藤', 'ö', '맑', '맏', 'ó', '常', 'ā', '藏', '口', '찐', 'μ', '슛', '벵', '玄', '任', '達', '꼈', '反', '텝', '楊', '몫', '許', '닦', '찔', '密', '甲', '場', '걀', '先', '鳳', '車', '옳', '對', '志', '깐', 'í', '亭', '뭇', '곶', '씌', '錫', '沈', '끗', '靈', '낫', '朱', '郎', '頭', '윅', '統', '쟈', '≪', '印', '≫', '夏', '禪', '얽', '佐', '랏', '熙', '麗', '烈', '姜', '팍', '出', '츄', '務', '뻔', '察', '團', '伯', '園', 'ν', '愛', '百', '識', '쭉', '榮', '器', '草', '關', '캅', '豊', '}', 'ι', '湖', '員', 'ς', '嘉', '現', '鄕', '튬', '儀', '귄', '諸', '뚱', '勝', '橋', '從', '意', '植', '晉', '商', '解', '墓', '■', '낚', '氣', '볍', '앰', '遠', 'ä', '祭', '鐵', '뜬', '○', '益', '千', '뱃', '容', '管', '뻐', '헛', '加', '竹', '伊', '起', '領', '坐', '챙', '쾰', '궈', '沙', '遺', '第', '팰', '찮', '調', '엮', '귤', '魏', '엥', '쩔', '恩', '秦', '處', '想', '機', '싹', '욘', '力', '非', 'ρ', '居', '雄', '―', 'し', '뾰', '報', '異', '畵', '핏', '렝', '뫼', '꿨', '泉', '惠', '次', 'ん', '캣', '柱', '鍾', '샐', '術', '黨', '火', 'β', '始', '我', 'τ', '支', '훙', '岩', '。', '崇', '뭄', '丁', '탬', '紀', '室', '쉴', '秋', '俊', '受', '得', '宇', '盧', '섀', 'か', '샀', '品', '웍', '廳', '昭', 'な', '資', '池', '깜', '類', '삐', '푹', 'う', '交', '얕', '낄', '넛', '果', '랬', '쿡', '싣', '舍', '뮈', '嚴', '營', '咸', '目', '育', '提', '돗', '꽂', '姓', '챈', 'ل', '曲', '늙', '系', '뜸', '局', '試', '總', 'ε', '믈', 'λ', '仲', '렙', '妃', '齊', '香', '@', '彦', '富', '卿', '圓', '尉', '少', 'ス', '顯', '能', '手', '波', '男', '表', '魚', '致', '꽝', '껏', '發', '댈', 'м', '群', '瑞', '냅', '慧', 'д', '吏', '숀', '戶', '防', '極', '角', '눔', '갚', '象', '農', '셴', 'た', '共', '母', '큘', '恭', '츰', '産', '情', '首', '쾨', '故', '署', '늪', '薩', '»', '촛', '짖', '邑', '즙', 'σ', 'と', '因', '췄', '蘇', '샵', '썩', '훤', '博', '☆', '짚', '尊', '樞', '탭', '虎', '覺', '陸', '찢', '△', '庵', '仙', 'й', '町', '結', '峰', '牧', '電', '갯', '듐', '쪼', '入', '房', '릅', 'イ', '몹', 'ル', '딧', '陰', '區', 'у', '半', '펭', '뿜', '獻', '蓮', '췌', '協', '倉', '屬', '淑', '閣', '녜', '빽', '完', '뜰', 'è', '域', '蔡', '뇽', '巖', '贊', '딥', '룻', '板', '겟', '範', '쁨', '슌', '괌', '굶', '溫', '翼', '菩', 'り', '普', '呂', '씻', '靖', '숍', '變', '～', '펩', '勳', '可', '쩐', '깁', '父', '넉', '乙', '連', '−', '쿰', 'る', '鶴', '맬', '哲', '엎', '銀', '丹', '퀄', '퀼', '샷', '邊', '僧', '死', '督', '뮐', '엡', '律', '燕', '핼', '学', '貴', '壇', '«', '船', '越', '祠', '台', '友', '淵', '뭐', '珍', '復', 'κ', '衆', '彌', '淳', '職', '載', '輔', '久', '벙', '—', '斗', '탔', '볶', '舊', '港', '롄', '\\\\', '藝', '近', '郭', '샴', '슝', 'ラ', '젯', '喜', '境', '莊', '쵸', '隆', '種', '赤', '奇', '잼', '껑', '벚', 'に', '具', '똥', '꿰', '뜯', '足', '윽', '允', '精', '臨', '썰', '衣', '땀', '澤', '純', '舞', '層', '特', '禹', '잣', '튠', '毛', '禁', '座', 'ま', '煥', '퓌', '球', '둡', '촘', '爲', '둬', '亞', '惡', '旨', '龜', '′', '素', '訓', '均', '맴', 'ら', '住', '낡', '뵈', '뽀', '촐', '陀', '雨', '奎', '쥘', '址', '廟', '洋', '멩', '쑨', 'リ', '儒', '꼼', '洙', '養', '劇', '條', '誠', '期', '轉', '텅', 'ト', '깅', '盛', '蘭', '閔', '魯', 'き', '備', '射', '隨', '괜', '멱', '浩', '私', '욤', '댕', '慈', '낯', '퓰', 'π', '聲', '丘', '内', '執', '€', '牛', '警', '좡', '今', '隊', '刑', '葉', '藩', '要', '念', '隱', '醫', '켐', '那', '放', '젓', '쥔', '以', '麻', '緣', '取', '킵', '列', '岳', '弼', '兼', 'γ', '淨', '뎀', '됩', '綱', '츨', '獨', '팎', '胡', '布', '質', '兒', '岡', '戦', '至', '桂', '聯', '팻', '±', '愼', '持', '謙', '훅', '檢', '改', '穆', '雙', 'は', '祿', '肅', '靜', '量', '캇', '觸', '羽', '製', '嶺', '希', '然', '若', '밧', '冠', 'ッ', '딤', 'く', '狀', '茂', '밈', '亂', 'ن', '封', '譜', '웜', '쉰', '圭', '洛', '由', 'お', '캥', '戒', '苦', '몄', '離', '띈', '釋', '쏠', '比', '錦', '約', '計', '랠', '聞', '助', '惱', '望', '賀', '썸', '퀀', '夢', '補', '銅', '于', '汝', 'я', 'ي', 'ク', '亨', '孔', '件', '飛', '珠', '뇰', '單', '甫', 'さ', '팥', 'δ', '帶', '卷', '欲', '兩', '委', '裵', '쑹', '칫', '剛', '末', '鏡', '뱌', 'ア', '摠', '考', '藥', '엌', '‧', '^', '왁', '客', '寅', '餘', '孟', '敏', '辛', '問', '屋', '樹', '杜', '易', '清', '짠', '尾', '恒', '鳥', '型', '留', 'à', '眼', '準', 'ī', 'て', '向', '誌', '쏜', '챠', '慕', '헴', 'η', '夷', '廷', '樓', '쐐', 'ç', '宰', '麟', '뭔', '퍽', '克', '役', '祥', '端', '윔', '쿵', '꽁', '威', '紅', '格', '滿', '環', '갸', '폈', '丞', '求', '黑', '솥', '技', '影', '会', '梅', '好', '晋', '葛', '釜', '鉉', 'г', '烏', '宅', '僉', '翰', 'あ', '究', '細', 'ь', '食', '쨌', 'ر', 'م', '厚', '己', 'و', '演', '叔', '辰', '摩', '넴', '乘', '講', '丙', '諫', '먀', '秉', '酒', '덥', '衡', '잖', '回', '啓', '選', '題', '騎', '깡', '뺏', '指', '曺', '證', '財', '乾', 'ち', '則', '쩍', '徳', '筆', '돛', '煩', '遊', 'É', 'â', 'タ', '增', '斷', '邱', '簡', '茶', '滅', '妙', '祐', '骨', '服', '話', '炳', '假', '籍', '敦', '未', '雅', '욜', '懿', '桓', '略', '젬', '句', '댁', 'こ', 'だ', '散', '宜', '条', '點', '콴', '昇', 'С', '邪', '늠', '雜', '裕', '딛', 'み', '軒', '銘', '얌', '扶', '輪', 'ы', '눕', '央', 'ú', '遼', '巨', '續', '革', '歸', 'シ', '뛴', '設', '刺', '襄', '鎬', '툭', '坊', '旗', '雪', '季', '擧', '硏', '策', '登', '盟', '뎅', '賜', '찻', 'つ', '休', '振', '曆', '丸', '征', '倭', '庶', '難', '칵', '震', 'ド', '庭', '繼', '固', '寬', '殷', '핌', '積', '廉', 'ч', '依', '契', '庫', '翁', '쑥', '貪', '卞', '勞', '病', '뮴', 'θ', 'が', '健', '놉', '앳', 'ί', 'ᄀ', 'を', '罪', '送', '읊', '簿', '峴', '最', '紙', '웁', '́', '俗', '旌', '感', '願', 'ω', '株', '欽', '版', '超', 'п', '洲', '옅', '姬', '贈', '際', '뺨', '힉', '刻', '坡', '寫', '皮', '⁄', '注', '童', '땐', '볕', '캬', 'も', '夜', '消', '燮', '뛸', 'υ', '詞', '賓', '邦', '★', '紫', 'ά', '翊', '쉘', '爾', '琴', '뺀', '懷', '漏', '僕', '再', '勢', '逸', 'す', '梵', '룀', 'カ', '晩', 'マ', '旅', '組', 'ᄋ', '区', '匡', '潤', '蒙', '諦', '횃', 'ό', '徒', '盤', '鑑', 'Δ', '←', '程', '蓋', '追', '介', '팸', 'れ', '深', '鼎', 'б', '序', 'え', '甘', '速', '過', '陶', 'よ', '歷', '썹', '쭈', '切', '鐘', '∙', '才', '県', '亮', '惟', '納', '엷', '磨', '巡', '站', '뀔', '봅', '料', '뽕', '凉', '止', '症', '퀵', 'د', '譯', '掌', '殺', '浮', '옙', '構', '팁', 'ジ', 'ロ', '配', '落', '壤', '芳', '샛', '推', '楚', '웸', 'š', 'з', '存', '毅', '刊', '熊', '꼰', '斯', '磁', '勇', 'ñ', '習', '背', 'ł', '꿇', '텃', '畿', '碩', '編', '置', '蘊', '갠', '숄', '尼', '虛', '於', '灣', '露', '땡', '춧', '泳', 'х', 'レ', '宿', '戸', '넵', '施', '頼', 'ê', '也', '而', '袁', '奴', '疏', '染', '維', '述', '活', '織', '씹', 'キ', '텡', '嬪', '街', '깬', '쉼', 'け', '項', 'フ', '率', '鹿', '失', '岐', '켤', 'ب', '祚', '迦', '냇', '뎌', '評', '깼', '奏', '當', '짤', '튿', '勒', '将', '崎', '뻘', '壁', '峯', '稅', '鳴', 'К', '輝', '差', '帥', '熱', '真', '〜', '專', '弟', '與', '告', '墳', '鬼', 'ß', '긁', '何', '干', '幸', '揚', '導', '米', '坪', '潭', '睦', 'ニ', '倫', '規', '墨', '爭', '步', 'コ', '堅', '模', '階', '刀', '援', '꽉', '찼', '휼', 'オ', '標', '닙', '뱉', '펨', 'ᄂ', 'っ', '冬', '舜', 'А', '移', '貢', '徽', '血', '賊', '蹟', '쇤', '撰', '獄', '紋', '綠', '引', '壬', '燈', '耶', '댑', '享', '決', '錢', '텟', '폄', '幕', '渡', '謝', '꺽', '눗', 'ᄅ', '案', '兪', '眠', '視', '貫', '깍', 'じ', '害', '蔚', '랗', '윳', '홑', '吾', '耳', '賞', '鼓', '其', '嗣', '葬', 'ô', '午', '奈', '弓', '愚', '照', '邉', 'ᄃ', '勤', '般', '輸', '撫', '青', '줍', '紹', '裝', '▲', '阮', '段', '솝', 'で', '秘', '胤', '辭', '힝', 'ᄆ', '豆', '솅', 'æ', 'ı', '候', '坂', '讀', '限', '얏', '戱', '浪', '伽', '庚', '歲', '쏴', '伏', '됭', '젼', '函', '操', '教', 'ナ', '栗', 'ū', 'φ', 'М', '豫', '査', '降', '웡', '喪', '魔', '算', '쌈', '쭤', '附', '뢴', '劍', '篇', '鴻', 'ə', 'や', '堤', '微', '忍', '授', '쎄', 'わ', '兄', '喆', '齡', '챕', 'ᅳ', '帖', '芝', '薛', '쫒', '諡', '£', 'П', '関', '頂', '符', '륀', 'ウ', '뎃', '創', '禧', '뜩', '傅', '壯', '材', '庄', '稷', '破', '鬪', '춥', '曾', '顔', '唱', '塚', '睿', '瞋', '障', '빴', '是', '班', '韻', '뎬', '幀', '級', '惑', '映', '蜀', '衍', '哀', '肉', '号', '廢', '彩', '接', '赫', '寒', 'س', '優', '閭', '뵤', '쩡', 'テ', '收', '脈', 'ø', 'ć', '伐', '脫', '舌', '還', '驪', '놨', '森', '檀', '須', 'В', 'ノ', 'ミ', '淮', '董', '郵', 'ш', '─', '冊', '꿩', '텁', 'バ', '箕', '萊', '談', '⟫', 'ャ', '徵', '油', '땜', '숱', '佑', '攝', '経', '묀', '찹', '岸', '救', '牙', '鍊', '̊', '包', '桃', '菴', '巴', '癡', '碧', '展', '請', '맙', '헵', '⟪', '味', '尺', '孤', '幹', '敵', '꿉', '튕', '給', '齒', '씽', '宝', '漁', '稱', '終', '讓', '退', '匠', '卯', '舟', '蔵', '黎', '놔', 'エ', 'チ', '融', '랸', 'ã', '去', '婦', '疑', '댜', '륄', '젱', '찜', 'サ', '逆', '떴', '卓', '潘', '볜', '헹', 'ع', '梨', '뤽', '뽐', '亥', '絶', '跋', '넋', '좇', 'έ', '寂', '잿', '橫', '番', '雍', '例', '崖', '換', '駐', '鷄', 'ひ', '唯', '実', '必', '陣', '빡', '쌌', '審', '蕭', 'ت', '羊', '숯', '컹', '仕', '湯', 'モ', '屯', '默', '县', '強', '腹', '썽', 'ュ', '價', '示', '옻', '잰', '햐', 'ᅵ', '柴', '旭', '覆', '賦', '鹽', '셍', '巫', '彭', '酉', '〕', '輿', '얗', '了', '婆', '戊', '毘', '菊', '著', '귈', '묄', '便', '培', '愍', '胎', '꿋', 'ィ', '慢', '爵', '뻣', '숏', '他', '낵', '潮', '탸', 'ē', '俱', '祀', '껌', 'ō', '〔', '댔', '볏', '앎', '흄', '貨', '푀', 'め', '築', '航', 'Б', 'י', 'グ', '妻', '朔', '澄', '禎', '遂', '酸', '丈', 'χ', 'ц', '巳', '濬', 'Р', '砲', '쌔', '広', '뷸', '녘', '括', '泥', '牟', '굼', '밌', '븀', '핥', 'Đ', '□', '停', '打', '折', '癸', '蔣', '覽', 'ư', 'ば', '牌', '탯', 'ょ', '輕', 'ズ', 'ム', '片', '헷', '底', '及', '듦', '줏', 'ه', '仇', '複', '軌', '툇', '占', '幡', '莫', '衙', '쳇', '万', '稿', 'ハ', '楽', 'プ', '祝', 'ή', '拳', '沃', '着', '蟲', '鎌', '炎', '탉', '戴', '整', '泗', 'ガ', '裁', '똘', 'メ', '肖', '駿', '챗', '券', '尚', '짊', '\\xad', 'ブ', '拓', '殊', 'ă', 'ˈ', 'ᄉ', '升', '召', '嫡', '庸', '槐', '沢', '突', '筵', '討', '乳', '擊', '菜', '휜', '佳', '尋', '店', '桐', '놋', '彰', '杉', '梧', '短', '쨩', '禍', '范', '鼻', '測', '曉', '朗', '枝', '鎮', '除', '덫', '坤', '就', '敍', 'Α', 'ろ', '係', '抗', '遷', '갬', 'ë', 'ː', '災', 'ᄇ', 'ゃ', '傑', '勅', '奭', '彈', '戌', '淡', '炯', '訴', 'И', '宙', '彫', '訪', '註', '냑', 'ύ', '凡', 'Н', '丑', '競', '賈', '辨', '溝', '藍', '幢', '態', '早', '갭', 'า', '胞', '荷', '被', '悲', '抄', '暴', '漆', '虞', 'ح', '壺', '槃', '汗', '耆', '額', '●', '穴', '雷', 'デ', '昊', '渾', '笠', 'Π', '乱', '昧', '苑', '荊', '待', '氷', '豪', '履', '甁', '遍', '郷', '魂', 'č', '乃', '禦', '綜', '綾', '艦', '閑', '隋', '괭', '눙', '욧', '仰', '幽', '探', '核', '穀', '卜', '各', '婚', '鈴', '塘', '毒', '炭', '粉', '鉢', '췬', 'Г', '∞', '垂', '扈', '賴', '껀', '젭', 'Σ', '犯', '遞', '拜', '눴', '沖', '珪', '采', '귓', '亡', '側', '呼', '権', '沼', '砂', '賣', '넙', 'ş', '█', '悟', '捨', '涅', '租', '頌', '앴', '첵', 'ふ', '濃', '荒', '費', '遇', '닳', 'ו', '儉', '迎', '뉠', '쁠', '쿱', '悅', '熟', '蕃', '궂', '쭝', 'å', '云', '企', '浚', '薰', '넜', 'Á', 'ж', '་', '投', '来', '耕', 'Т', '為', '総', '肥', '陜', '뺄', 'ゆ', '伴', '供', '宴', '按', '絲', '謹', '走', '긱', 'ぶ', '応', '卽', '庾', '效', '曰', '跡', 'そ', '卒', '哈', '浜', '鏞', '튤', '懸', '矢', '認', '뷜', 'ダ', '余', '卑', '寛', '床', '排', '擇', '晴', '璋', '網', 'ビ', '低', '急', '暗', '更', '盡', '黄', '넝', '숴', 'ق', '姫', '屠', '뵐', 'ی', '幼', '杖', '樣', '驗', '늉', '뚤', '콸', '億', '謀', '軸', '陟', '깟', '伝', '僞', '筑', '縛', '諱', '쥰', 'パ', '席', '攻', '晶', '섣', 'Д', 'ה', 'ف', 'せ', 'ど', 'ワ', '倍', '宏', '息', '窟', '鎔', '兆', '堯', '菌', '諭', 'ᄌ', '勿', '灘', '鄧', '헉', '冷', '彬', '獸', '舒', '馮', '喬', '忘', '眉', '緖', '豐', '飯', '쳅', '팹', '免', '含', '播', '洗', '燦', '瓦', '꾹', '垣', '奮', '招', '瀬', '謨', '겜', '†', 'ず', '圈', '烽', '迪', 'Л', 'ة', '銃', '쟝', '짬', 'ᄒ', '又', '戎', '緯', '훠', 'ᄏ', '塾', '駕', '鷹', '뎠', '솽', '펍', '【', '】', '束', '答', '買', '頓', '髮', '돤', 'ר', '壓', '奥', '蔭', '衝', '뭘', 'ج', 'ツ', '呪', '課', '頃', 'Κ', '党', '冥', '慰', '負', '껄', '벰', 'Ö', '↔', '憂', '昆', '텨', 'О', '浙', '蒲', '詳', '윕', '东', '充', '損', '뀜', '몐', '셤', '왠', 'ョ', '忽', '扇', '蜜', '劑', '强', '架', '索', '聚', '黒', '涉', '混', 'ร', '体', '渠', '犬', '篆', '險', '뗀', 'ð', 'ơ', 'ェ', '荀', '阪', '寄', '悌', '掛', '湘', '爐', '辦', 'ŋ', 'ほ', 'セ', '押', '詔', '괵', '듄', '셧', '숟', 'Φ', '沿', '綏', '韋', '뗏', '쬐', '팠', 'ï', 'ケ', '偏', '寇', '往', '揮', '樑', '訥', '阜', '샥', '텼', '⋅', 'び', '迷', '雀', '믐', '뷴', '쫄', 'î', 'ý', '瞻', '鑄', '빤', '푈', '伸', '嶽', '潛', '灌', '疾', '筒', '羲', '겅', '떳', '偉', '対', '戚', '拉', '糖', '訟', '顧', '맣', '왑', '此', '澈', '箭', '켠', 'ò', 'ệ', '付', '忌', '桑', '絃', '챘', '堀', '帽', '悼', '沒', '甄', '笑', '鉄', '镇', '雕', '닻', '씰', 'む', '傷', '屛', '幾', '溟', '練', '襲', '왐', '参', '綿', '諺', '適', '只', '姚', '晦', '満', '쏙', '챤', '≤', 'ね', '乞', '柔', '栢', '樊', '蒼', '観', '鑛', '끽', '숑', 'ś', 'א', '々', '꼿', 'ヶ', '갉', '뎁', '似', '广', '捕', '橘', '覇', '鵬', 'Ε', 'أ', 'ボ', '堡', '蘆', '蛇', '쪄', '刹', '郁', 'ž', '廊', '浅', '繫', '耽', '샅', '햅', '吹', '渓', '僖', '娘', '巾', '斤', '燧', '畫', '療', '线', '貝', '遮', '隣', '霞', 'э', 'ю', 'น', '冀', '脚', '誤', '꿍', '쥴', '톳', 'Å', 'ァ', '塵', '已', '看', '祉', '綬', '訣', '違', '넹', '뺑', '뻑', 'ᅡ', '旦', '游', '纂', '芸', '짼', '쳄', '휨', 'Š', 'ա', 'げ', '慮', '琉', '翔', 'ネ', '値', '劃', '斥', '暦', '瑜', '膺', 'ù', '借', '勸', '吐', '峻', '曜', '減', '燁', '皆', '窯', '膜', '零', '従', '循', '晟', '狄', '猛', '璿', '禄', '互', '圃', '幻', '燒', '獅', '瓊', '盜', '萱', '讚', '뭍', '債', '婢', '裏', '譚', '貿', '鳩', 'ế', '円', '屈', '歐', '漫', '皐', '趣', '鐸', '岑', '旋', '갛', 'ش', 'ヤ', '勉', '婁', '杏', '杓', '汀', '煙', '礙', '稽', '竿', '賤', '遣', '馨', '割', '卵', '塞', '変', '寵', '怪', '斬', '横', '狂', '珉', '避', '땔', '묽', '짰', '칡', 'ベ', '揆', '敞', '榜', '琳', '盆', '翠', '闕', '넸', '्', '恪', '挺', '擬', '旺', '棋', '睡', '翟', '裴', 'へ', 'ゅ', '亀', '抱', '湛', '粹', '솁', '쉔', '힛', 'ì', 'ń', '伎', '弐', '衿', '껫', '톄', 'ψ', 'ك', 'र', 'ソ', '姑', '弁', '怡', '昔', '杞', '畢', '確', '礪', '祈', '肇', '趾', '週', '預', 'ğ', 'Λ', 'ё', '誘', '閤', '륌', '폿', '徹', '殉', '遵', '邵', '閉', '飾', '빳', '쩨', '쳉', 'Ω', 'َ', '凝', '媛', '恐', '斎', '票', '辯', '閻', '켁', '斜', '熹', '画', '畜', '縱', '藻', '遜', '냘', '횔', 'ф', '個', '倒', '寿', '廻', '析', '沛', '炫', '窩', '腦', '説', '龐', '냔', 'ě', '♪', '串', '廬', '恵', '涼', '濱', '繁', '纏', '邸', '욥', '잽', '튈', 'ز', 'ᄎ', 'ᅩ', 'ḥ', '叢', '妄', '帳', '湜', '盈', '祗', '羌', '聰', '肩', '駒', '뒬', 'Μ', 'ξ', '„', '竺', '腸', '託', '録', '®', 'У', 'ב', '卦', '壞', '拔', '杭', '渭', '糧', '脂', '謁', '퉈', 'ạ', 'ピ', '奧', '峽', '快', '柏', '漸', '隸', '顕', '섐', '탰', 'ᄑ', '妓', '朋', '札', '棟', '檜', '渤', '珥', '畠', '秩', '詠', '鴨', '빻', 'Ü', 'і', '‚', 'ヒ', '凰', '峙', '灰', '猿', '瑩', '繡', '脩', '詮', '謠', '铁', 'ぎ', '圍', '崗', '敷', '溶', '脣', '苻', '醴', '隅', '좨', '쭐', 'מ', '场', '幣', '弥', '捷', '皓', '績', '誓', '陝', '녓', '씁', '윷', '죤', 'Γ', 'ご', '否', '彼', '悔', '滉', '狗', '窮', '褒', '輯', '鈞', '鎖', '뢸', 'ा', '什', '叉', '棺', '牒', '猶', '逢', '鄒', '銓', '閱', '▶', 'ぬ', '倂', '冶', '凶', '患', '液', '瀛', '疇', '筋', '薄', '裂', '阳', '냠', '찧', 'ζ', 'ṭ', 'ễ', '夕', '尤', '拘', '柄', '璽', '需', '響', '뗄', '∎', '勃', '戀', '楷', '歡', '溥', '軾', '鎰', '駅', '윰', '予', '伺', '採', '紗', '詹', '針', '飮', '黔', '깰', '멎', '틋', 'ܐ', '寸', '崙', '拾', '獲', '祇', '禿', '蠶', '遲', '鋼', '飡', '亦', '滋', '疫', '硬', '꽈', '뵙', '셩', '웩', '츤', 'ל', 'ა', 'ザ', 'ポ', '吸', '巢', '恨', '據', '暉', '楞', '电', '畏', '誕', 'ʼ', 'ก', 'ᄐ', '僚', '嵌', '椒', '爆', '瓚', '研', '篤', '胄', '螺', 'ʿ', 'ъ', '倻', '償', '哉', '爀', '狼', '窓', 'ง', '与', '你', '凌', '危', '坦', '塑', '弗', '弱', '械', '棲', '澗', '誼', '뜀', '솀', 'ɪ', 'Ο', 'อ', '♭', 'ぐ', '滄', '苗', '趺', '밉', '뺐', '‰', 'ゴ', '冕', '団', '旻', '瑛', '穡', '辺', '잴', '쫑', 'ώ', 'ᅮ', '偶', '厭', '当', '殘', '滑', '瓜', '竟', '臧', '醉', '隴', '믄', '웽', '쿈', '훑', 'Β', '図', '幅', '恋', '愧', '暎', '槍', '粒', '縮', '臥', '虹', '衰', '醮', '霖', '퍄', '§', 'Č', 'đ', 'ʻ', 'Ф', 'ი', 'ṣ', 'ギ', '凱', '叱', '幷', '批', '晃', '栄', '每', '気', '琦', '禾', '諍', '辟', '鞍', '韶', '驅', '쐈', '쾡', '홰', 'Θ', 'ม', 'ホ', '兎', '峨', '曼', '沆', '瀑', '燃', '罰', '聽', '腫', '腺', '莞', '鱗', '뷧', 'ط', '丕', '兢', '到', '媒', '悳', '掾', '旬', '曇', '汚', '涵', '牡', '猫', '璧', '郊', '髻', '鬱', '龙', 'З', 'べ', 'ヴ', '墩', '拍', '敗', '旣', '椿', '沔', '点', '籠', '継', '耀', '豹', '貸', '閩', '鞠', '쇳', '쟌', '刷', '哥', '懺', '昕', '楓', '痛', '矣', '礎', '竇', '虜', '鄴', '겊', '꽌', '垈', '尸', '慚', '楠', '絹']\n"
          ]
        }
      ],
      "source": [
        "# 특수 token 7개를 제외한 나머지 tokens 들\n",
        "vocab_list = []\n",
        "for id in range(7, len(vocab)):\n",
        "    if not vocab.is_unknown(id):\n",
        "        vocab_list.append(vocab.id_to_piece(id))\n",
        "print(vocab_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eca06435",
      "metadata": {
        "id": "eca06435"
      },
      "source": [
        "# 2. 데이터 전처리 (1) MASK 생성\n",
        "\n",
        "- BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. 그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e021321",
      "metadata": {
        "id": "2e021321"
      },
      "outputs": [],
      "source": [
        "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
        "    \"\"\"\n",
        "    마스크 생성\n",
        "    :param tokens: tokens\n",
        "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
        "    :param vocab_list: vocab list (random token 용)\n",
        "    :return tokens: mask된 tokens\n",
        "    :return mask_idx: mask된 token의 index\n",
        "    :return mask_label: mask된 token의 원래 값\n",
        "    \"\"\"\n",
        "    # 단어 단위로 mask 하기 위해서 index 분할\n",
        "    cand_idx = []  # word 단위의 index array\n",
        "    for (i, token) in enumerate(tokens):\n",
        "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
        "            continue\n",
        "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
        "            cand_idx[-1].append(i)\n",
        "        else:\n",
        "            cand_idx.append([i])\n",
        "    # random mask를 위해서 순서를 섞음\n",
        "    random.shuffle(cand_idx)\n",
        "\n",
        "    mask_lms = []  # mask 된 값\n",
        "    for index_set in cand_idx:\n",
        "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
        "            break\n",
        "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
        "            continue\n",
        "        dice = random.random()  # 0..1 사이의 확률 값\n",
        "        for index in index_set:\n",
        "            masked_token = None\n",
        "            if dice < 0.8:  # 80% replace with [MASK]\n",
        "                masked_token = \"[MASK]\"\n",
        "            elif dice < 0.9: # 10% keep original\n",
        "                masked_token = tokens[index]\n",
        "            else:  # 10% random word\n",
        "                masked_token = random.choice(vocab_list)\n",
        "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
        "            tokens[index] = masked_token\n",
        "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
        "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
        "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
        "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
        "\n",
        "    return tokens, mask_idx, mask_label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "643354d7",
      "metadata": {
        "id": "643354d7"
      },
      "source": [
        "# 3. 데이터 전처리 (2) NSP pair 생성\n",
        "\n",
        "- BERT의 pretrain task인 NSP는 두 문장이 연속하는지 확인하는 것입니다. 이를 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 지정해 주세요.\n",
        "\n",
        "- 두 문장 사이에 segment 처리를 해주세요. 첫 번째 문장의 segment는 0, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주세요.\n",
        "\n",
        "- MLM과 NSP는 동시에 학습된다는 것을 염두에 두고 학습 데이터를 구성해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33513358",
      "metadata": {
        "id": "33513358"
      },
      "outputs": [],
      "source": [
        "# 최대 길이\n",
        "n_test_seq = 64\n",
        "# 최소 길이\n",
        "min_seq = 8\n",
        "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
        "max_seq = n_test_seq - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e7d758",
      "metadata": {
        "id": "30e7d758"
      },
      "outputs": [],
      "source": [
        "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
        "    \"\"\"\n",
        "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
        "    :param tokens_a: tokens A\n",
        "    :param tokens_b: tokens B\n",
        "    :param max_seq: 두 tokens 길이의 최대 값\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_seq:\n",
        "            break\n",
        "\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            del tokens_a[0]\n",
        "        else:\n",
        "            tokens_b.pop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d92c230b",
      "metadata": {
        "id": "d92c230b"
      },
      "source": [
        "### create_pretrain_instances() : Next Sentence Prediction을 위한 코퍼스 생성 메소드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02821f12",
      "metadata": {
        "id": "02821f12"
      },
      "outputs": [],
      "source": [
        "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
        "    \"\"\"\n",
        "    doc별 pretrain 데이터 생성\n",
        "    \"\"\"\n",
        "    # for CLS], [SEP], [SEP]\n",
        "    max_seq = n_seq - 3\n",
        "\n",
        "    instances = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for i in range(len(doc)):\n",
        "        current_chunk.append(doc[i])  # line\n",
        "        current_length += len(doc[i])\n",
        "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
        "            # token a\n",
        "            a_end = 1\n",
        "            if 1 < len(current_chunk):\n",
        "                a_end = random.randrange(1, len(current_chunk))\n",
        "            tokens_a = []\n",
        "            for j in range(a_end):\n",
        "                tokens_a.extend(current_chunk[j])\n",
        "            # token b\n",
        "            tokens_b = []\n",
        "            for j in range(a_end, len(current_chunk)):\n",
        "                tokens_b.extend(current_chunk[j])\n",
        "\n",
        "            if random.random() < 0.5:  # 50% 확률로 swap\n",
        "                is_next = 0\n",
        "                tokens_t = tokens_a\n",
        "                tokens_a = tokens_b\n",
        "                tokens_b = tokens_t\n",
        "            else:\n",
        "                is_next = 1\n",
        "            # max_seq 보다 큰 경우 길이 조절\n",
        "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
        "            assert 0 < len(tokens_a)\n",
        "            assert 0 < len(tokens_b)\n",
        "            # tokens & aegment 생성\n",
        "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
        "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
        "            # mask\n",
        "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
        "\n",
        "            instance = {\n",
        "                \"tokens\": tokens,\n",
        "                \"segment\": segment,\n",
        "                \"is_next\": is_next,\n",
        "                \"mask_idx\": mask_idx,\n",
        "                \"mask_label\": mask_label\n",
        "            }\n",
        "            instances.append(instance)\n",
        "\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "    return instances"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03dc0376",
      "metadata": {
        "id": "03dc0376"
      },
      "source": [
        "# 4. 데이터 전처리 (3) 데이터셋 완성\n",
        "\n",
        "- BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. 데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화해 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0a270c",
      "metadata": {
        "id": "8c0a270c",
        "outputId": "8617f962-ce06-4da6-f75b-0774d64d2ce2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3957761"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
        "\n",
        "# line count 확인\n",
        "total = 0\n",
        "with open(corpus_file, 'r') as in_f:\n",
        "    for line in in_f:\n",
        "        total += 1\n",
        "\n",
        "total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3863be6",
      "metadata": {
        "id": "a3863be6"
      },
      "source": [
        "(1) 주제별 document 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86a7d623",
      "metadata": {
        "collapsed": true,
        "id": "86a7d623",
        "outputId": "38a0838e-a4fd-4a26-8145-77f035040a1b",
        "colab": {
          "referenced_widgets": [
            "9ad8f3f0d853469fbdf1c38e71e0e96b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ad8f3f0d853469fbdf1c38e71e0e96b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3957761 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21 lines : ['▁지', '미', '▁카', '터']\n",
            "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
            "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
            "\n",
            "14 lines : ['▁수학']\n",
            "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
            "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
            "\n",
            "4 lines : ['▁수학', '▁상', '수']\n",
            "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
            "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
            "\n",
            "10 lines : ['▁문학']\n",
            "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
            "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
            "\n",
            "10 lines : ['▁나라', '▁목록']\n",
            "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
            "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
            "\n",
            "['▁화학']\n",
            "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
            "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
          ]
        }
      ],
      "source": [
        "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
        "count = 5\n",
        "\n",
        "with open(corpus_file, 'r') as in_f:\n",
        "    doc = []  # 단락 단위로 문서 저장\n",
        "    for line in tqdm(in_f, total=total):\n",
        "        line = line.strip()\n",
        "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
        "            if 0 < len(doc):\n",
        "                if 0 < count:\n",
        "                    count -= 1\n",
        "                    print(len(doc), \"lines :\", doc[0])\n",
        "                    print(doc[1])\n",
        "                    print(doc[-1])\n",
        "                    print()\n",
        "                else:\n",
        "                    break\n",
        "                doc = []\n",
        "        else:  # 빈 줄이 아니면 doc에 저장\n",
        "            pieces = vocab.encode_as_pieces(line)    \n",
        "            if 0 < len(pieces):\n",
        "                doc.append(pieces)\n",
        "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
        "        print(doc[0])\n",
        "        print(doc[1])\n",
        "        print(doc[-1])\n",
        "        doc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6d8409e",
      "metadata": {
        "collapsed": true,
        "id": "e6d8409e",
        "outputId": "75119faf-5cea-44fa-a5c6-dc91bbdef094",
        "colab": {
          "referenced_widgets": [
            "9bebdfcff8814c6195393af39a35e565"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bebdfcff8814c6195393af39a35e565",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3957761 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doc: 21 instances: 10\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 53, 54, 55, 56], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물', '은']}\n",
            "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 28, 29, 30, 31, 40, 50, 57, 58], 'mask_label': ['으로', '▁다루', '어진', '다', '.', '▁이루어진', '▁원래', '▁식물', '이나']}\n",
            "\n",
            "doc: 14 instances: 7\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '틋', '▁크게', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 44, 45, 51, 52], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁연구', '하는', '▁유', '기']}\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 8, 9, 33, 34, 35, 36, 37], 'mask_label': ['▁플', '라스', '틱', ',', '▁유', '기', '화', '학', '은']}\n",
            "\n",
            "doc: 4 instances: 2\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [40, 41, 42, 43, 46, 47, 48, 49, 50], 'mask_label': ['▁이루어진', '▁화', '합', '물을', '▁분', '과', '이다', '.', '▁원래']}\n",
            "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 4, 5, 51, 52, 61, 62], 'mask_label': ['으로', '▁자리', '잡', '았다', '.', '▁유', '기', '▁추', '출']}\n",
            "\n",
            "doc: 10 instances: 5\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 8, 9, 21, 22, 40, 44, 45], 'mask_label': ['▁플', '라스', '틱', ',', '▁등', '도', '▁이루어진', '▁연구', '하는']}\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [21, 22, 38, 39, 40, 57, 58, 61, 62], 'mask_label': ['▁등', '도', '▁탄', '소로', '▁이루어진', '▁식물', '이나', '▁추', '출']}\n",
            "\n",
            "doc: 10 instances: 5\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [21, 22, 23, 24, 25, 26, 27, 61, 62], 'mask_label': ['▁등', '도', '▁유', '기', '화', '학', '에서', '▁추', '출']}\n",
            "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 40, 44, 45, 50, 51, 52, 61, 62], 'mask_label': ['으로', '▁이루어진', '▁연구', '하는', '▁원래', '▁유', '기', '▁추', '출']}\n",
            "\n",
            "doc: 31 instances: 15\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [16, 17, 18, 19, 20, 44, 45, 51, 52], 'mask_label': ['▁고', '분', '자', '물', '질', '▁연구', '하는', '▁유', '기']}\n",
            "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 8, 9, 46, 47, 48, 49, 50], 'mask_label': ['▁플', '라스', '틱', ',', '▁분', '과', '이다', '.', '▁원래']}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# instance 생성 기능 확인\n",
        "count = 5\n",
        "\n",
        "with open(corpus_file, 'r') as in_f:\n",
        "    doc = []  # 단락 단위로 문서 저장\n",
        "    for line in tqdm(in_f, total=total):\n",
        "        line = line.strip()\n",
        "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
        "            if 0 < len(doc):\n",
        "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
        "                # save\n",
        "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
        "                print(instances[0])\n",
        "                print(instances[-1])\n",
        "                print()\n",
        "                doc = []\n",
        "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
        "                    count -= 1\n",
        "                else:\n",
        "                    break\n",
        "        else:  # doc에 저장\n",
        "            if 0 < len(pieces):\n",
        "                doc.append(pieces)\n",
        "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
        "        instances = create_pretrain_instances(doc, 128)\n",
        "        # save\n",
        "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
        "        print(instances[0])\n",
        "        print(instances[-1])\n",
        "        print()\n",
        "        doc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74912e23",
      "metadata": {
        "id": "74912e23"
      },
      "outputs": [],
      "source": [
        "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
        "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
        "    def save_pretrain_instances(out_f, doc):\n",
        "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
        "        for instance in instances:\n",
        "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
        "            out_f.write(\"\\n\")\n",
        "\n",
        "    # 특수문자 7개를 제외한 vocab_list 생성\n",
        "    vocab_list = []\n",
        "    for id in range(7, len(vocab)):\n",
        "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
        "            vocab_list.append(vocab.id_to_piece(id))\n",
        "\n",
        "    # line count 확인\n",
        "    line_cnt = 0\n",
        "    with open(in_file, \"r\") as in_f:\n",
        "        for line in in_f:\n",
        "            line_cnt += 1\n",
        "\n",
        "    with open(in_file, \"r\") as in_f:\n",
        "        with open(out_file, \"w\") as out_f:\n",
        "            doc = []\n",
        "            for line in tqdm(in_f, total=line_cnt):\n",
        "                line = line.strip()\n",
        "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
        "                    if 0 < len(doc):\n",
        "                        save_pretrain_instances(out_f, doc)\n",
        "                        doc = []\n",
        "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
        "                    pieces = vocab.encode_as_pieces(line)\n",
        "                    if 0 < len(pieces):\n",
        "                        doc.append(pieces)\n",
        "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
        "                save_pretrain_instances(out_f, doc)\n",
        "                doc = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac902afc",
      "metadata": {
        "id": "ac902afc",
        "outputId": "eb799461-3964-419c-ae56-5378880d329b",
        "colab": {
          "referenced_widgets": [
            "067567b2bcf14621919c5348e7d52eab"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "067567b2bcf14621919c5348e7d52eab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3957761 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train_mini.json'\n",
        "\n",
        "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a31987cf",
      "metadata": {
        "id": "a31987cf",
        "outputId": "1cedf475-64c5-4e1b-bba6-b0e59f338fbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "918189"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total = 0\n",
        "with open(pretrain_json_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        total += 1\n",
        "total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c92416",
      "metadata": {
        "id": "b3c92416",
        "outputId": "34c2e001-2677-42a2-c76f-e01c8529df85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
              " 0,\n",
              " 0,\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
              " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_seq = 128\n",
        "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
        "max_seq = n_seq - 3\n",
        "\n",
        "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
        "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
        "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
        "# labels_nsp = np.zeros((total,), np.int32)\n",
        "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
        "\n",
        "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
        "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
        "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "\n",
        "\n",
        "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74ef986",
      "metadata": {
        "id": "d74ef986",
        "outputId": "f59b74d5-6f2a-47bb-8108-ba54cbd4275d",
        "colab": {
          "referenced_widgets": [
            "f2c8184e9cf9493f8993a38abdb71f24"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2c8184e9cf9493f8993a38abdb71f24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/918189 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'tokens': ['[CLS]', '[MASK]', '▁~', '[MASK]', '[MASK]', '▁민주', '당', '▁출신', '[MASK]', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.', '[MASK]', '[MASK]', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '[MASK]', '[MASK]', '▁태어났다', '.', '[MASK]', '[MASK]', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '[MASK]', '▁후', '[MASK]', '[MASK]', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '[MASK]', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '[MASK]', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.', '[SEP]', '▁지', '미', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 3, 4, 8, 23, 24, 39, 40, 43, 44, 52, 54, 55, 77, 90, 119, 120, 121], 'mask_label': ['일', '▁)', '는', '▁미국', '▁지', '미', '▁마을', '에서', '▁조지', '아', '▁그', '▁해', '군에', '▁미국', '▁등을', '▁알려', '졌다', '.']}\n",
            "enc_token: [5, 6, 203, 6, 6, 1114, 3724, 788, 6, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 203, 3008, 3625, 3616, 16, 3599, 6, 6, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 6, 6, 1605, 3599, 6, 6, 41, 3644, 830, 3624, 1135, 52, 3599, 6, 81, 6, 6, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 6, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 6, 21, 5007, 399, 1927, 3607, 813, 17, 3599, 307, 587, 931, 103, 4313, 4290, 613, 3638, 3718, 98, 3878, 3656, 256, 2543, 309, 337, 3735, 181, 3616, 3603, 489, 376, 3599, 4, 18, 3686, 207, 3714, 4]\n",
            "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "label_nsp: 0\n",
            "label_mlm: [   0 3629    0  241 3602    0    0    0  243    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0   18 3686    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0 1369   10    0\n",
            "    0 1755 3630    0    0    0    0    0    0    0   13    0   87 1501\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0  243    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0  593    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0  489  376 3599    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '[MASK]', '[MASK]', '[MASK]', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '[MASK]', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '[MASK]', '[MASK]', '▁되', '기', '[MASK]', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '▁사는', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]', '[MASK]', '[MASK]', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '[MASK]', '[MASK]', '[MASK]', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '▁당선', '되었다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 4, 16, 17, 18, 31, 39, 40, 49, 50, 53, 99, 100, 111, 112, 113], 'mask_label': ['아', '▁주', '▁상', '원', '▁', '였', '음을', '▁주', '▁1970', '년', '▁대통령', '이', '▁전', '▁1976', '년', '▁도', '덕', '주의']}\n",
            "enc_token: [5, 6, 6, 6, 6, 2378, 822, 10, 1567, 3668, 3294, 13, 822, 3608, 2386, 2163, 6, 6, 6, 213, 3929, 173, 607, 2387, 317, 3604, 386, 3673, 3625, 1755, 3630, 6, 18, 3620, 822, 3600, 1567, 3668, 1447, 6, 6, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 6, 6, 450, 3614, 6, 1755, 3630, 3646, 76, 955, 928, 157, 3821, 61, 3773, 530, 3604, 3372, 523, 3409, 673, 1755, 3630, 18, 982, 2711, 31, 3599, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 243, 3600, 3554, 1733, 3628, 50, 3717, 2046, 114, 3692, 1853, 3599, 4, 6, 6, 663, 822, 3600, 1114, 3724, 958, 3603, 117, 3674, 54, 6, 6, 6, 1421, 9, 114, 3692, 3964, 3604, 119, 1486, 807, 2056, 2387, 43, 3599, 4]\n",
            "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_nsp: 1\n",
            "label_mlm: [   0 3630   37   76 3667    0    0    0    0    0    0    0    0    0\n",
            "    0    0 3596 3671  969    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0   37    0    0    0    0    0    0    0 1921 3625    0\n",
            "    0    0    0    0    0    0    0  663 3597    0    0   25    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0 3306 3625    0    0    0    0    0    0    0    0    0    0   75\n",
            " 4089  238    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "{'tokens': ['[CLS]', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '[MASK]', '[MASK]', '[MASK]', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '[MASK]', '[MASK]', '▁제한', '▁협', '상에', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '[MASK]', '[MASK]', '[MASK]', '▁국민', '들의', '[MASK]', '[MASK]', '▁지', '키', '기', '[MASK]', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '[SEP]', '▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '[MASK]', '[MASK]', '[MASK]', '▁반', '대로', '▁무', '산', '되었다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [43, 44, 45, 60, 61, 65, 66, 67, 68, 80, 81, 82, 85, 86, 90, 118, 119, 120], 'mask_label': ['▁양', '국', '▁간의', '▁무', '기', '▁조', '인', '했다', '.', '▁후', '진', '국의', '▁인', '권을', '▁위해', '▁공', '화', '당의']}\n",
            "enc_token: [5, 3644, 280, 35, 3658, 232, 934, 521, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 330, 1487, 41, 3683, 3724, 3644, 679, 2670, 3628, 164, 1314, 141, 3720, 3607, 1213, 4174, 3598, 3599, 2995, 3625, 456, 3928, 3708, 10, 6, 6, 6, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 1302, 3644, 30, 3619, 3751, 2835, 6, 6, 1956, 617, 1824, 6, 6, 6, 6, 207, 3714, 3602, 1921, 596, 1840, 316, 410, 50, 42, 3830, 6, 6, 6, 968, 247, 6, 6, 18, 3793, 3614, 6, 3375, 530, 3604, 2659, 165, 785, 874, 75, 4089, 3642, 1233, 114, 3692, 1853, 3599, 4, 207, 3714, 663, 3613, 1778, 3610, 570, 3607, 2270, 3653, 1003, 6, 6, 6, 141, 448, 107, 3726, 43, 3599, 4]\n",
            "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_nsp: 0\n",
            "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0  230 3643 2714    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0  107 3614    0    0    0   53 3628   31 3599    0\n",
            "    0    0    0    0    0    0    0    0    0    0   81 3713  137    0\n",
            "    0   42  917    0    0    0  231    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0   41 3683 1547    0    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "{'tokens': ['[CLS]', '▁반', '대에', '[MASK]', '[MASK]', '[MASK]', '▁주', '한', '미', '군은', '▁완', '전', '철', '수', '▁대신', '▁6', ',000', '명을', '▁감', '축', '하는', '▁데', '▁그', '쳤다', '.', '▁또한', '▁박', '정', '희', '▁정', '권의', '▁인', '권', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁', '냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국', '을', '▁방문', '하여', '▁관계', '가', '▁다', '소', '▁회복', '되었다', '.', '[SEP]', '▁그러나', '▁주', '▁이란', '▁미국', '▁대사', '관', '▁인', '질', '▁사건', '에서', '▁인', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '[MASK]', '[MASK]', '[MASK]', '▁레이', '건', '▁후보', '에게', '[MASK]', '[MASK]', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '[MASK]', '[MASK]', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '▁인해', '[MASK]', '[MASK]', '▁하계', '▁올림픽', '에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 3, 4, 5, 14, 26, 27, 28, 90, 91, 92, 97, 98, 112, 113, 122, 123], 'mask_label': ['▁반', '대에', '▁부', '딪', '혀', '▁대신', '▁박', '정', '희', '▁로', '널', '드', '▁', '져', '▁소련', '의', '▁1980', '년']}\n",
            "enc_token: [5, 141, 867, 6, 6, 6, 37, 3612, 3686, 941, 443, 3640, 3917, 3636, 1083, 125, 847, 859, 209, 3909, 38, 189, 13, 1523, 3599, 276, 338, 3642, 4055, 36, 2649, 42, 3830, 550, 50, 786, 2408, 9, 128, 3993, 3683, 969, 3596, 4121, 191, 3604, 2995, 3625, 125, 3662, 27, 3946, 3604, 410, 3607, 2017, 54, 704, 3608, 29, 3688, 3332, 43, 3599, 4, 330, 37, 3290, 243, 2630, 3708, 42, 3892, 636, 10, 42, 3892, 73, 3771, 1579, 3624, 1827, 1640, 3625, 663, 822, 10, 41, 3683, 1547, 6, 6, 6, 1169, 3803, 958, 113, 6, 6, 875, 174, 2087, 1579, 31, 3599, 276, 273, 3614, 150, 329, 870, 3713, 6, 6, 26, 2986, 3733, 1323, 3232, 636, 9, 751, 6, 6, 2219, 779, 3600, 4]\n",
            "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_nsp: 0\n",
            "label_mlm: [   0  141  867   51 5148 4178    0    0    0    0    0    0    0    0\n",
            " 1083    0    0    0    0    0    0    0    0    0    0    0  338 3642\n",
            " 4055    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0  194 4044 3681    0    0    0    0 3596\n",
            " 3944    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            " 1302 3601    0    0    0    0    0    0    0    0 1640 3625    0    0\n",
            "    0    0]\n",
            "\n",
            "{'tokens': ['[CLS]', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '▁위해', '▁제', '▁3', '세', '계의', '▁선거', '▁감', '시', '▁활동', '▁및', '▁기', '니', '▁벌', '레', '에', '썽', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '층', '▁지원', '▁활동', ',', '▁사랑', '의', '▁집', '짓', '기', '▁운동', ',', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.', '[SEP]', '▁1979', '년', '▁~', '▁1980', '년', '▁대한민국의', '▁정치적', '[MASK]', '[MASK]', '[MASK]', '▁당시의', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '[MASK]', '[MASK]', '[MASK]', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고', '조', '되는', '▁반', '미', '▁운동', '의', '[MASK]', '▁원', '인이', '▁', '됐다', '.', '[MASK]', '[MASK]', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '▁중앙', '정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 28, 29, 72, 73, 74, 86, 87, 88, 100, 106, 107, 114, 115], 'mask_label': ['▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁격', '변', '기', '▁보', '였고', ',', '▁한', '▁10', '월', '▁대통령', '이']}\n",
            "enc_token: [5, 3612, 339, 1114, 238, 158, 3756, 3607, 231, 30, 49, 3692, 1654, 822, 209, 3623, 375, 228, 24, 3733, 813, 3740, 3600, 6677, 6, 6, 6, 6, 6, 6, 761, 3886, 95, 3729, 3624, 231, 947, 4437, 3598, 3599, 679, 1412, 4234, 4083, 770, 375, 3604, 1424, 3601, 313, 4333, 3614, 887, 3604, 605, 147, 3972, 35, 3729, 507, 375, 3627, 345, 3599, 4, 2995, 3625, 203, 1640, 3625, 447, 2843, 6, 6, 6, 3195, 663, 1277, 202, 695, 433, 442, 3823, 3612, 227, 701, 6, 6, 6, 594, 1140, 410, 3428, 70, 3676, 267, 141, 3686, 887, 3601, 6, 129, 828, 3596, 1027, 3599, 6, 6, 981, 3629, 3604, 338, 3642, 4055, 663, 3597, 200, 3729, 3958, 782, 2275, 3638, 1312, 355, 2591, 3711, 2057, 4]\n",
            "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_nsp: 0\n",
            "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0 1332  311 3635 4956 3937\n",
            " 3699 3626    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0 1032 3889 3614    0    0    0    0    0    0    0    0    0\n",
            "    0    0   47 2470 3604    0    0    0    0    0    0    0    0    0\n",
            "    0    0   34    0    0    0    0    0  131 3662    0    0    0    0\n",
            "    0    0  663 3597    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "{'tokens': ['[CLS]', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '벰', '▁및', '▁단', '체를', '▁직접', '[MASK]', '▁분', '쟁', '의', '▁원', '인을', '▁근', '본', '적으로', '▁해결', '하기', '▁위해', '▁힘', '썼', '다', '.', '▁이', '▁과정에서', '▁미국', '▁행정', '부와', '▁갈', '등', '을', '▁보', '이기도', '▁했지만', ',', '▁전', '직', '▁대통령', '의', '▁권', '한', '과', '▁재', '야', '[MASK]', '[MASK]', '[MASK]', '했을', '干', '▁해결', '해', '▁나', '갔다', '.', '[SEP]', '▁카', '터', '는', '▁카', '터', '▁행정', '부', '▁이후', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '▁이', '라크', '▁전쟁', '과', '▁같이', '▁미국', '이', '▁군사', '적', '▁행', '동을', '[MASK]', '[MASK]', '[MASK]', '▁선택', '하는', '▁전통', '적', '▁사고', '를', '▁버', '리고', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁선', '행', '하는', '▁행', '위에', '▁대해', '▁깊', '은', '▁유', '감을', '[MASK]', '[MASK]', '[MASK]', '▁군사', '적', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 17, 54, 55, 56, 57, 58, 96, 97, 98, 107, 108, 109, 110, 121, 122, 123, 126], 'mask_label': ['▁인물', '▁만나', '▁유명', '▁인사', '들의', '▁활약', '으로', '▁최', '후', '로', '▁군사', '적', '▁행', '동을', '▁표시', '▁하며', '▁미국의', '▁활동']}\n",
            "enc_token: [5, 460, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 7196, 228, 164, 1396, 1069, 6, 147, 3972, 3601, 129, 1171, 387, 3759, 127, 2317, 167, 231, 947, 4437, 3598, 3599, 8, 2208, 243, 895, 2576, 742, 3709, 3607, 47, 1304, 3379, 3604, 25, 3802, 663, 3601, 476, 3612, 3644, 174, 3775, 6, 6, 6, 1723, 6326, 2317, 3645, 58, 1133, 3599, 4, 207, 3714, 3602, 207, 3714, 895, 3638, 165, 243, 3597, 251, 4166, 45, 3614, 3604, 258, 3688, 3672, 506, 3604, 8, 3553, 506, 3644, 733, 243, 3597, 1250, 3657, 236, 1629, 6, 6, 6, 1715, 38, 1306, 3657, 1646, 3624, 407, 999, 6, 6, 6, 6, 57, 3752, 38, 236, 1157, 433, 1910, 3613, 46, 2196, 6, 6, 6, 1250, 3657, 6, 4]\n",
            "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "label_nsp: 0\n",
            "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0 1178    0\n",
            "    0    0    0 2142    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0  939 3329\n",
            "  247 1102    9    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0  130 3706\n",
            " 3603    0    0    0    0    0    0    0    0 1250 3657  236 1629    0\n",
            "    0    0    0    0    0    0    0    0    0 2466 1368  679    0    0\n",
            "  375    0]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_77/767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
            "/tmp/ipykernel_77/767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
            "/tmp/ipykernel_77/767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
          ]
        }
      ],
      "source": [
        "# 라인 단위로 처리\n",
        "with open(pretrain_json_path, \"r\") as f:\n",
        "    for i, line in enumerate(tqdm(f, total=total)):\n",
        "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
        "            break\n",
        "        data = json.loads(line)\n",
        "        # encoder token\n",
        "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
        "        enc_token += [0] * (n_seq - len(enc_token))\n",
        "        # segment\n",
        "        segment = data[\"segment\"]\n",
        "        segment += [0] * (n_seq - len(segment))\n",
        "        # nsp label\n",
        "        label_nsp = data[\"is_next\"]\n",
        "        # mlm label\n",
        "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
        "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
        "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
        "        label_mlm[mask_idx] = mask_label\n",
        "\n",
        "        print(data)\n",
        "        print(\"enc_token:\", enc_token)\n",
        "        print(\"segment:\", segment)\n",
        "        print(\"label_nsp:\", label_nsp)\n",
        "        print(\"label_mlm:\", label_mlm)\n",
        "        print()\n",
        "\n",
        "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
        "\n",
        "        enc_tokens[i] = enc_token\n",
        "        segments[i] = segment\n",
        "        labels_nsp[i] = label_nsp\n",
        "        labels_mlm[i] = label_mlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d076585c",
      "metadata": {
        "id": "d076585c"
      },
      "outputs": [],
      "source": [
        "# load_pre_train_data() : 학습에 필요한 데이터를 로딩하는 함수\n",
        "\n",
        "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
        "    \"\"\"\n",
        "    학습에 필요한 데이터를 로드\n",
        "    :param vocab: vocab\n",
        "    :param filename: 전처리된 json 파일\n",
        "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
        "    :param count: 데이터 수 제한 (None이면 전체)\n",
        "    :return enc_tokens: encoder inputs\n",
        "    :return segments: segment inputs\n",
        "    :return labels_nsp: nsp labels\n",
        "    :return labels_mlm: mlm labels\n",
        "    \"\"\"\n",
        "    total = 0\n",
        "    with open(filename, \"r\") as f:\n",
        "        for line in f:\n",
        "            total += 1\n",
        "            # 데이터 수 제한\n",
        "            if count is not None and count <= total:\n",
        "                break\n",
        "    \n",
        "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
        "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
        "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
        "\n",
        "    with open(filename, \"r\") as f:\n",
        "        for i, line in enumerate(tqdm(f, total=total)):\n",
        "            if total <= i:\n",
        "                print(\"data load early stop\", total, i)\n",
        "                break\n",
        "            data = json.loads(line)\n",
        "            # encoder token\n",
        "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
        "            enc_token += [0] * (n_seq - len(enc_token))\n",
        "            # segment\n",
        "            segment = data[\"segment\"]\n",
        "            segment += [0] * (n_seq - len(segment))\n",
        "            # nsp label\n",
        "            label_nsp = data[\"is_next\"]\n",
        "            # mlm label\n",
        "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
        "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
        "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
        "            label_mlm[mask_idx] = mask_label\n",
        "\n",
        "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
        "\n",
        "            enc_tokens[i] = enc_token\n",
        "            segments[i] = segment\n",
        "            labels_nsp[i] = label_nsp\n",
        "            labels_mlm[i] = label_mlm\n",
        "\n",
        "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2a5a9c",
      "metadata": {
        "id": "2f2a5a9c",
        "outputId": "8f31bf36-585d-4256-e1d4-bc54ad4587dc",
        "colab": {
          "referenced_widgets": [
            "86ecc86e073141f9993d314462ebbadf"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86ecc86e073141f9993d314462ebbadf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/128000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_77/2733615032.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
            "/tmp/ipykernel_77/2733615032.py:45: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
            "/tmp/ipykernel_77/2733615032.py:46: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data load early stop 128000 128000\n"
          ]
        }
      ],
      "source": [
        "# 128000건만 메모리에 로딩\n",
        "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27fdc0c",
      "metadata": {
        "id": "d27fdc0c"
      },
      "source": [
        "# 5. BERT 모델 구현\n",
        "\n",
        "- pad mask, ahead mask 함수, gelu activation 함수, parameter initializer 생성 함수, json을 config 형태로 사용하기 위한 유틸리티 함수를 먼저 만들어 두세요.\n",
        "\n",
        "- Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de887244",
      "metadata": {
        "id": "de887244"
      },
      "source": [
        "유틸리티 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71ccb32",
      "metadata": {
        "id": "d71ccb32"
      },
      "outputs": [],
      "source": [
        "def get_pad_mask(tokens, i_pad=0):\n",
        "    \"\"\"\n",
        "    pad mask 계산하는 함수\n",
        "    :param tokens: tokens (bs, n_seq)\n",
        "    :param i_pad: id of pad\n",
        "    :return mask: pad mask (pad: 1, other: 0)\n",
        "    \"\"\"\n",
        "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
        "    mask = tf.expand_dims(mask, axis=1)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def get_ahead_mask(tokens, i_pad=0):\n",
        "    \"\"\"\n",
        "    ahead mask 계산하는 함수\n",
        "    :param tokens: tokens (bs, n_seq)\n",
        "    :param i_pad: id of pad\n",
        "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
        "    \"\"\"\n",
        "    n_seq = tf.shape(tokens)[1]\n",
        "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
        "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
        "    pad_mask = get_pad_mask(tokens, i_pad)\n",
        "    mask = tf.maximum(ahead_mask, pad_mask)\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58a3cbbd",
      "metadata": {
        "id": "58a3cbbd"
      },
      "outputs": [],
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    gelu activation 함수\n",
        "    :param x: 입력 값\n",
        "    :return: gelu activation result\n",
        "    \"\"\"\n",
        "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f6a6994",
      "metadata": {
        "id": "3f6a6994"
      },
      "outputs": [],
      "source": [
        "def kernel_initializer(stddev=0.02):\n",
        "    \"\"\"\n",
        "    parameter initializer 생성\n",
        "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
        "    \"\"\"\n",
        "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
        "\n",
        "\n",
        "def bias_initializer():\n",
        "    \"\"\"\n",
        "    bias initializer 생성\n",
        "    \"\"\"\n",
        "    return tf.zeros_initializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "716a7ada",
      "metadata": {
        "id": "716a7ada"
      },
      "outputs": [],
      "source": [
        "class Config(dict):\n",
        "    \"\"\"\n",
        "    json을 config 형태로 사용하기 위한 Class\n",
        "    :param dict: config dictionary\n",
        "    \"\"\"\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        \"\"\"\n",
        "        file에서 Config를 생성 함\n",
        "        :param file: filename\n",
        "        \"\"\"\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c86d0a5",
      "metadata": {
        "id": "0c86d0a5"
      },
      "outputs": [],
      "source": [
        "class SharedEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Weighed Shaed Embedding Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.n_vocab = config.n_vocab\n",
        "        self.d_model = config.d_model\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        shared weight 생성\n",
        "        :param input_shape: Tensor Shape (not used)\n",
        "        \"\"\"\n",
        "        with tf.name_scope(\"shared_embedding_weight\"):\n",
        "            self.shared_weights = self.add_weight(\n",
        "                \"weights\",\n",
        "                shape=[self.n_vocab, self.d_model],\n",
        "                initializer=kernel_initializer()\n",
        "            )\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\"):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: 입력\n",
        "        :param mode: 실행 모드\n",
        "        :return: embedding or linear 실행 결과\n",
        "        \"\"\"\n",
        "        # mode가 embedding일 경우 embedding lookup 실행\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs)\n",
        "        # mode가 linear일 경우 linear 실행\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        # mode가 기타일 경우 오류 발생\n",
        "        else:\n",
        "            raise ValueError(f\"mode {mode} is not valid.\")\n",
        "    \n",
        "    def _embedding(self, inputs):\n",
        "        \"\"\"\n",
        "        embedding lookup\n",
        "        :param inputs: 입력\n",
        "        \"\"\"\n",
        "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
        "        return embed\n",
        "\n",
        "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
        "        \"\"\"\n",
        "        linear 실행\n",
        "        :param inputs: 입력\n",
        "        \"\"\"\n",
        "        n_batch = tf.shape(inputs)[0]\n",
        "        n_seq = tf.shape(inputs)[1]\n",
        "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
        "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
        "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f02597c",
      "metadata": {
        "id": "9f02597c"
      },
      "outputs": [],
      "source": [
        "class PositionEmbedding(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Position Embedding Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"position_embedding\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: 입력\n",
        "        :return embed: position embedding lookup 결과\n",
        "        \"\"\"\n",
        "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
        "        embed = self.embedding(position)\n",
        "        return embed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e65a55c",
      "metadata": {
        "id": "2e65a55c"
      },
      "source": [
        "ScaleDotProductAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f6f65aa",
      "metadata": {
        "id": "8f6f65aa"
      },
      "outputs": [],
      "source": [
        "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Scale Dot Product Attention Class\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param Q: Q value\n",
        "        :param K: K value\n",
        "        :param V: V value\n",
        "        :param attn_mask: 실행 모드\n",
        "        :return attn_out: attention 실행 결과\n",
        "        \"\"\"\n",
        "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
        "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
        "        attn_scale = tf.math.divide(attn_score, scale)\n",
        "        attn_scale -= 1.e9 * attn_mask\n",
        "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
        "        attn_out = tf.matmul(attn_prob, V)\n",
        "        return attn_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b81c1a1",
      "metadata": {
        "id": "4b81c1a1"
      },
      "source": [
        "MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f64cc05",
      "metadata": {
        "id": "6f64cc05"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Multi Head Attention Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"multi_head_attention\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.d_model = config.d_model\n",
        "        self.n_head = config.n_head\n",
        "        self.d_head = config.d_head\n",
        "\n",
        "        # Q, K, V input dense layer\n",
        "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        # Scale Dot Product Attention class\n",
        "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
        "        # output dense layer\n",
        "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "\n",
        "    def call(self, Q, K, V, attn_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param Q: Q value\n",
        "        :param K: K value\n",
        "        :param V: V value\n",
        "        :param attn_mask: 실행 모드\n",
        "        :return attn_out: attention 실행 결과\n",
        "        \"\"\"\n",
        "        # reshape Q, K, V, attn_mask\n",
        "        batch_size = tf.shape(Q)[0]\n",
        "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
        "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
        "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
        "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
        "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
        "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
        "        # transpose and liner\n",
        "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
        "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
        "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
        "\n",
        "        return attn_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69b4f313",
      "metadata": {
        "id": "69b4f313"
      },
      "source": [
        "transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcaada3",
      "metadata": {
        "id": "4dcaada3"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Position Wise Feed Forward Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"feed_forward\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: inputs\n",
        "        :return ff_val: feed forward 실행 결과\n",
        "        \"\"\"\n",
        "        ff_val = self.W_2(self.W_1(inputs))\n",
        "        return ff_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5724c0fb",
      "metadata": {
        "id": "5724c0fb"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Encoder Layer Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"encoder_layer\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(config)\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForward(config)\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
        " \n",
        "    def call(self, enc_embed, self_mask):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
        "        :param self_mask: enc_tokens의 pad mask\n",
        "        :return enc_out: EncoderLayer 실행 결과\n",
        "        \"\"\"\n",
        "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
        "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
        "\n",
        "        ffn_val = self.ffn(norm1_val)\n",
        "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
        "\n",
        "        return enc_out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beab2ef4",
      "metadata": {
        "id": "beab2ef4"
      },
      "source": [
        "BERT 레이어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d1279c",
      "metadata": {
        "id": "87d1279c"
      },
      "outputs": [],
      "source": [
        "class BERT(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    BERT Class\n",
        "    \"\"\"\n",
        "    def __init__(self, config, name=\"bert\"):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param config: Config 객체\n",
        "        :param name: layer name\n",
        "        \"\"\"\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.i_pad = config.i_pad\n",
        "        self.embedding = SharedEmbedding(config)\n",
        "        self.position = PositionEmbedding(config)\n",
        "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
        "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
        "        \n",
        "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        layer 실행\n",
        "        :param inputs: (enc_tokens, segments)\n",
        "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
        "        \"\"\"\n",
        "        enc_tokens, segments = inputs\n",
        "\n",
        "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
        "\n",
        "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
        "\n",
        "        enc_out = self.dropout(enc_embed)\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
        "\n",
        "        logits_cls = enc_out[:,0]\n",
        "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
        "        return logits_cls, logits_lm\n",
        "    \n",
        "    def get_embedding(self, tokens, segments):\n",
        "        \"\"\"\n",
        "        token embedding, position embedding lookup\n",
        "        :param tokens: 입력 tokens\n",
        "        :param segments: 입력 segments\n",
        "        :return embed: embedding 결과\n",
        "        \"\"\"\n",
        "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
        "        embed = self.norm(embed)\n",
        "        return embed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "973c9e20",
      "metadata": {
        "id": "973c9e20"
      },
      "source": [
        "BERT 레이어를 바탕으로 최종적으로 만들어질 pretrain용 BERT 모델 구성은 아래와 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60322b8",
      "metadata": {
        "id": "f60322b8"
      },
      "outputs": [],
      "source": [
        "# Encoder Layer class 정의\n",
        "class PooledOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
        " \n",
        "    def call(self, inputs):\n",
        "        outputs = self.dense1(inputs)\n",
        "        outputs = self.dense2(outputs)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8667cc8f",
      "metadata": {
        "id": "8667cc8f"
      },
      "outputs": [],
      "source": [
        "def build_model_pre_train(config):\n",
        "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
        "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
        "\n",
        "    bert = BERT(config)\n",
        "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
        "\n",
        "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
        "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
        "\n",
        "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
        "\n",
        "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92865fd1",
      "metadata": {
        "id": "92865fd1"
      },
      "source": [
        "아주 작은 pretrain용 BERT 모델(test_model)을 생성하여 동작을 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "905ec60f",
      "metadata": {
        "scrolled": true,
        "id": "905ec60f",
        "outputId": "46fdce52-e0c4-4e1c-8948-a18c86d2463c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'d_model': 64,\n",
              " 'n_head': 4,\n",
              " 'd_head': 64,\n",
              " 'dropout': 0.1,\n",
              " 'd_ff': 1024,\n",
              " 'layernorm_epsilon': 0.001,\n",
              " 'n_layer': 3,\n",
              " 'n_seq': 256,\n",
              " 'n_vocab': 8007,\n",
              " 'i_pad': 0}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = Config({\"d_model\": 64, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
        "config.n_vocab = len(vocab)\n",
        "config.i_pad = vocab.pad_id()\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29cd2e3f",
      "metadata": {
        "id": "29cd2e3f",
        "outputId": "2bded2ac-2650-4548-a9c7-9417e261ad87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 6s 11ms/step - loss: 9.6972 - nsp_loss: 0.6932 - mlm_loss: 9.0040 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9.4655 - nsp_loss: 0.6863 - mlm_loss: 8.7792 - nsp_acc: 0.8000 - mlm_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 9.2684 - nsp_loss: 0.6842 - mlm_loss: 8.5842 - nsp_acc: 1.0000 - mlm_acc: 0.0900\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 9.0728 - nsp_loss: 0.6769 - mlm_loss: 8.3959 - nsp_acc: 0.9000 - mlm_acc: 0.1800\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.8660 - nsp_loss: 0.6670 - mlm_loss: 8.1990 - nsp_acc: 1.0000 - mlm_acc: 0.3400\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.6627 - nsp_loss: 0.6540 - mlm_loss: 8.0087 - nsp_acc: 1.0000 - mlm_acc: 0.4600\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.4626 - nsp_loss: 0.6235 - mlm_loss: 7.8390 - nsp_acc: 1.0000 - mlm_acc: 0.5800\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.2308 - nsp_loss: 0.5818 - mlm_loss: 7.6490 - nsp_acc: 1.0000 - mlm_acc: 0.6300\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 7.9910 - nsp_loss: 0.5312 - mlm_loss: 7.4598 - nsp_acc: 1.0000 - mlm_acc: 0.5600\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7.7551 - nsp_loss: 0.4632 - mlm_loss: 7.2920 - nsp_acc: 1.0000 - mlm_acc: 0.6500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f93a8f71100>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_seq = 10\n",
        "\n",
        "# make test inputs\n",
        "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
        "segments = np.random.randint(0, 2, (10, n_seq))\n",
        "labels_nsp = np.random.randint(0, 2, (10,))\n",
        "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
        "\n",
        "test_model = build_model_pre_train(config)\n",
        "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
        "\n",
        "# test model fit\n",
        "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=10, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e3d838",
      "metadata": {
        "id": "40e3d838"
      },
      "source": [
        "# 6. pretrain 진행\n",
        "\n",
        "- loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요. 학습을 진행할 때는 배치 사이즈에 유의하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3c2f78",
      "metadata": {
        "id": "3b3c2f78"
      },
      "source": [
        "loss와 accuracy같이 기본적으로 필요한 계산 함수를 미리 정의해 둡시다. 학습 데이터의 label이 정수로 변환되었으므로 loss 함수는 SparseCategoricalCrossentropy를 사용합니다. MLM task에 대해 더 잘 학습하도록 loss를 20배 증가시켜 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b83fcd6",
      "metadata": {
        "id": "2b83fcd6"
      },
      "outputs": [],
      "source": [
        "def lm_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    loss 계산 함수\n",
        "    :param y_true: 정답 (bs, n_seq)\n",
        "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
        "    \"\"\"\n",
        "    # loss 계산\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
        "    # pad(0) 인 부분 mask\n",
        "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c75e92",
      "metadata": {
        "id": "40c75e92"
      },
      "outputs": [],
      "source": [
        "def lm_acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    acc 계산 함수\n",
        "    :param y_true: 정답 (bs, n_seq)\n",
        "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
        "    \"\"\"\n",
        "    # 정답 여부 확인\n",
        "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
        "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
        "    # pad(0) 인 부분 mask\n",
        "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
        "    matches *= mask\n",
        "    # 정확도 계산\n",
        "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fee4626f",
      "metadata": {
        "id": "fee4626f"
      },
      "source": [
        "Learning Rate 스케줄링"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64c5f5f9",
      "metadata": {
        "id": "64c5f5f9"
      },
      "outputs": [],
      "source": [
        "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"\n",
        "    CosineSchedule Class\n",
        "    \"\"\"\n",
        "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
        "        \"\"\"\n",
        "        생성자\n",
        "        :param train_steps: 학습 step 총 합\n",
        "        :param warmup_steps: warmup steps\n",
        "        :param max_lr: 최대 learning rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        assert 0 < warmup_steps < train_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.train_steps = train_steps\n",
        "        self.max_lr = max_lr\n",
        "\n",
        "    def __call__(self, step_num):\n",
        "        \"\"\"\n",
        "        learning rate 계산\n",
        "        :param step_num: 현재 step number\n",
        "        :retrun: 계산된 learning rate\n",
        "        \"\"\"\n",
        "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
        "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
        "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
        "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
        "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de32543",
      "metadata": {
        "id": "3de32543",
        "outputId": "efa517bf-b3b1-4d84-ed46-a048023559ef"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# compute lr \n",
        "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
        "lrs = []\n",
        "for step_num in range(4000):\n",
        "    lrs.append(test_schedule(float(step_num)).numpy())\n",
        "\n",
        "# draw\n",
        "plt.plot(lrs, 'r-', label='learning_rate')\n",
        "plt.xlabel('Step')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd5fa0b",
      "metadata": {
        "id": "1dd5fa0b"
      },
      "source": [
        "이제 모델을 실제로 빌드해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b70189b",
      "metadata": {
        "id": "4b70189b",
        "outputId": "3b0b67a9-a9c7-4015-ff5d-f32966cada9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segments (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert (BERT)                     ((None, 64), (None,  1125440     enc_tokens[0][0]                 \n",
            "                                                                 segments[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pooled_nsp (PooledOutput)       (None, 2)            4288        bert[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
            "==================================================================================================\n",
            "Total params: 1,129,728\n",
            "Trainable params: 1,129,728\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 모델 생성\n",
        "pre_train_model = build_model_pre_train(config)\n",
        "pre_train_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebde2df2",
      "metadata": {
        "id": "ebde2df2"
      },
      "source": [
        "학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d8a1888",
      "metadata": {
        "id": "6d8a1888",
        "outputId": "5b54c4d4-19c5-4381-dc1f-c481e312b148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_steps: 40000\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "# optimizer\n",
        "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
        "print(\"train_steps:\", train_steps)\n",
        "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "# compile\n",
        "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0033c517",
      "metadata": {
        "id": "0033c517",
        "outputId": "34528c6f-fbd8-4b5d-8518-710f8bc2a74a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2000/2000 [==============================] - 167s 82ms/step - loss: 21.5404 - nsp_loss: 0.6811 - mlm_loss: 20.8593 - nsp_acc: 0.5378 - mlm_lm_acc: 0.0400\n",
            "\n",
            "Epoch 00001: mlm_lm_acc improved from -inf to 0.03999, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 2/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 19.4144 - nsp_loss: 0.6430 - mlm_loss: 18.7714 - nsp_acc: 0.6021 - mlm_lm_acc: 0.0479\n",
            "\n",
            "Epoch 00002: mlm_lm_acc improved from 0.03999 to 0.04786, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 3/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 18.3902 - nsp_loss: 0.6332 - mlm_loss: 17.7570 - nsp_acc: 0.6154 - mlm_lm_acc: 0.0955\n",
            "\n",
            "Epoch 00003: mlm_lm_acc improved from 0.04786 to 0.09550, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 4/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 17.7719 - nsp_loss: 0.6239 - mlm_loss: 17.1480 - nsp_acc: 0.6271 - mlm_lm_acc: 0.1150\n",
            "\n",
            "Epoch 00004: mlm_lm_acc improved from 0.09550 to 0.11501, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 5/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 17.4255 - nsp_loss: 0.6180 - mlm_loss: 16.8075 - nsp_acc: 0.6367 - mlm_lm_acc: 0.1238\n",
            "\n",
            "Epoch 00005: mlm_lm_acc improved from 0.11501 to 0.12376, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 6/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 17.2082 - nsp_loss: 0.6109 - mlm_loss: 16.5973 - nsp_acc: 0.6454 - mlm_lm_acc: 0.1278\n",
            "\n",
            "Epoch 00006: mlm_lm_acc improved from 0.12376 to 0.12784, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 7/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 17.0466 - nsp_loss: 0.6082 - mlm_loss: 16.4384 - nsp_acc: 0.6493 - mlm_lm_acc: 0.1306\n",
            "\n",
            "Epoch 00007: mlm_lm_acc improved from 0.12784 to 0.13056, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 8/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.9069 - nsp_loss: 0.6063 - mlm_loss: 16.3006 - nsp_acc: 0.6525 - mlm_lm_acc: 0.1327\n",
            "\n",
            "Epoch 00008: mlm_lm_acc improved from 0.13056 to 0.13270, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 9/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.7782 - nsp_loss: 0.6055 - mlm_loss: 16.1728 - nsp_acc: 0.6561 - mlm_lm_acc: 0.1347\n",
            "\n",
            "Epoch 00009: mlm_lm_acc improved from 0.13270 to 0.13467, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 10/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.6606 - nsp_loss: 0.6038 - mlm_loss: 16.0568 - nsp_acc: 0.6581 - mlm_lm_acc: 0.1364\n",
            "\n",
            "Epoch 00010: mlm_lm_acc improved from 0.13467 to 0.13640, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 11/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.5532 - nsp_loss: 0.6028 - mlm_loss: 15.9504 - nsp_acc: 0.6618 - mlm_lm_acc: 0.1380\n",
            "\n",
            "Epoch 00011: mlm_lm_acc improved from 0.13640 to 0.13802, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 12/20\n",
            "2000/2000 [==============================] - 166s 83ms/step - loss: 16.4524 - nsp_loss: 0.6022 - mlm_loss: 15.8501 - nsp_acc: 0.6629 - mlm_lm_acc: 0.1396\n",
            "\n",
            "Epoch 00012: mlm_lm_acc improved from 0.13802 to 0.13959, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 13/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.3602 - nsp_loss: 0.6009 - mlm_loss: 15.7593 - nsp_acc: 0.6652 - mlm_lm_acc: 0.1412\n",
            "\n",
            "Epoch 00013: mlm_lm_acc improved from 0.13959 to 0.14117, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 14/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.2840 - nsp_loss: 0.5999 - mlm_loss: 15.6840 - nsp_acc: 0.6676 - mlm_lm_acc: 0.1423\n",
            "\n",
            "Epoch 00014: mlm_lm_acc improved from 0.14117 to 0.14225, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 15/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.2221 - nsp_loss: 0.5988 - mlm_loss: 15.6233 - nsp_acc: 0.6692 - mlm_lm_acc: 0.1432\n",
            "\n",
            "Epoch 00015: mlm_lm_acc improved from 0.14225 to 0.14318, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 16/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.1701 - nsp_loss: 0.5979 - mlm_loss: 15.5722 - nsp_acc: 0.6718 - mlm_lm_acc: 0.1439\n",
            "\n",
            "Epoch 00016: mlm_lm_acc improved from 0.14318 to 0.14392, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 17/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.1337 - nsp_loss: 0.5972 - mlm_loss: 15.5365 - nsp_acc: 0.6726 - mlm_lm_acc: 0.1446\n",
            "\n",
            "Epoch 00017: mlm_lm_acc improved from 0.14392 to 0.14461, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 18/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.1073 - nsp_loss: 0.5959 - mlm_loss: 15.5114 - nsp_acc: 0.6743 - mlm_lm_acc: 0.1450\n",
            "\n",
            "Epoch 00018: mlm_lm_acc improved from 0.14461 to 0.14501, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 19/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.0928 - nsp_loss: 0.5960 - mlm_loss: 15.4968 - nsp_acc: 0.6746 - mlm_lm_acc: 0.1451\n",
            "\n",
            "Epoch 00019: mlm_lm_acc improved from 0.14501 to 0.14512, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n",
            "Epoch 20/20\n",
            "2000/2000 [==============================] - 165s 83ms/step - loss: 16.0859 - nsp_loss: 0.5953 - mlm_loss: 15.4906 - nsp_acc: 0.6753 - mlm_lm_acc: 0.1452\n",
            "\n",
            "Epoch 00020: mlm_lm_acc improved from 0.14512 to 0.14516, saving model to /aiffel/aiffel/bert_pre_train.hdf5\n"
          ]
        }
      ],
      "source": [
        "# save weights callback\n",
        "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
        "# train\n",
        "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6e9fc3",
      "metadata": {
        "id": "bc6e9fc3"
      },
      "source": [
        "# 7. 프로젝트 결과\n",
        "\n",
        "- 학습된 모델과 학습과정을 시각화해 보세요. NSP와 MLM의 loss가 안정적으로 수렴하나요? 모델이 작기 때문에 loss가 잘 수렴하지 않을 수도 있어요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c07d7e",
      "metadata": {
        "id": "91c07d7e",
        "outputId": "c542a0dd-ed79-42a6-e3c3-fa617259dd16"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# training result\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
        "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
        "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65c8a2a",
      "metadata": {
        "id": "e65c8a2a"
      },
      "source": [
        "# 회고\n",
        "\n",
        "1. 어려웠던 점\n",
        "- Bert를 직접 구현해보는 과정이 쉽지 않았다. 향후 논문을 한번 더 참고하요 이해를 할 필요가 있다고 본다\n",
        "\n",
        "2. 알게된 점\n",
        "- d-model을 조정해서 파라미터 값을 조정할 수 있음을 알게되었다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}